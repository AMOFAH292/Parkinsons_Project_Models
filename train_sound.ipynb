{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaee5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b802ba7e-84ea-4bfa-b380-2de402491772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROOT = Path(\"PARK_DATA\")\n",
    "OUTPUT_ROOT = Path(\"dataset_5sec\")\n",
    "OUTPUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "(HC_OUT := OUTPUT_ROOT / \"HC\").mkdir(exist_ok=True)\n",
    "(PD_OUT := OUTPUT_ROOT / \"PD\").mkdir(exist_ok=True)\n",
    "\n",
    "SEGMENT_LENGTH_SEC = 5\n",
    "SR = 16000  # YAMNet expects 16 kHz mono\n",
    "\n",
    "def split_and_save(audio_path, label_folder):\n",
    "    audio, _ = librosa.load(audio_path, sr=SR, mono=True)\n",
    "    segment_samples = SEGMENT_LENGTH_SEC * SR\n",
    "    total_segments = len(audio) // segment_samples\n",
    "    \n",
    "    for i in range(total_segments):\n",
    "        start = i * segment_samples\n",
    "        end = start + segment_samples\n",
    "        segment = audio[start:end]\n",
    "        \n",
    "        # Only save segments that are exactly 5 seconds\n",
    "        if len(segment) == segment_samples:\n",
    "            out_path = (HC_OUT if \"HC\" in str(label_folder) else PD_OUT) / f\"{audio_path.stem}_{i:04d}.wav\"\n",
    "            sf.write(out_path, segment, SR)\n",
    "\n",
    "# Process all files\n",
    "for folder in [\"HC_segments\", \"PD_segments\"]:\n",
    "    label_path = ROOT / folder\n",
    "    for wav_file in label_path.rglob(\"*.wav\"):\n",
    "        split_and_save(wav_file, label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de3e0f-1b81-404c-8f23-d06319bdaf73",
   "metadata": {},
   "source": [
    "### Training Yamnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0e849f-c85d-414c-89c9-bf158f678f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'waveforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m yamnet \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/yamnet/1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ------------------------------- 3. Train/val split -------------------------------\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m---> 15\u001b[0m     waveforms, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mlabels, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'waveforms' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------- 1. Load YAMNet (supports batching in 2024+) -------------------------------\n",
    "yamnet = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "--------------------- 4. Extract YAMNet embeddings (batched & fast) -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18cd33f-3fd9-4a6d-86d5-4a1c53cfa63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully: 644 samples, 43.63% PD\n"
     ]
    }
   ],
   "source": [
    "@tf.function  # This makes it fast and handles single waveforms perfectly\n",
    "def extract_single_embedding(waveform):\n",
    "    \"\"\"Extracts averaged embedding for ONE 5-sec waveform.\"\"\"\n",
    "    scores, embeddings, spectrograms = yamnet(waveform)\n",
    "    return tf.reduce_mean(embeddings, axis=0, keepdims=True)  # (1, 1024)\n",
    "\n",
    "# ------------------------------- 3. Load your 5-second segments -------------------------------\n",
    "def load_data(data_dir):\n",
    "    files = list(Path(data_dir).rglob(\"*.wav\"))\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    for f in files:\n",
    "        waveform, _ = librosa.load(f, sr=16000, mono=True)\n",
    "        # Force exactly 5 seconds (80000 samples) – truncate or pad\n",
    "        target_len = 80000\n",
    "        if len(waveform) > target_len:\n",
    "            waveform = waveform[:target_len]\n",
    "        elif len(waveform) < target_len:\n",
    "            waveform = np.pad(waveform, (0, target_len - len(waveform)), mode='constant')\n",
    "        waveforms.append(waveform)\n",
    "        labels.append(0 if \"HC\" in str(f) else 1)  # 0 = Healthy Control, 1 = Parkinson's\n",
    "    return np.array(waveforms, dtype=np.float32), np.array(labels)  # Ensure float32\n",
    "\n",
    "print(\"Loading data...\")\n",
    "waveforms, labels = load_data(\"dataset_5sec\")\n",
    "print(f\"Data loaded successfully: {waveforms.shape[0]} samples, {np.mean(labels):.2%} PD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d254c7f0-4e4f-4cbe-a145-74ae1f689564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- 3. Train/val split -------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    waveforms, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a7c143-0ad6-4158-ac2e-67a9d953534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting YAMNet embeddings (parallel processing – 1-3 min)...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000016300FF2020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000016300FF2020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000016300FF2020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000016300FF2020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings ready: train (515, 1024), val (129, 1024)\n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings_vectorized(waveforms_np, batch_size=32):\n",
    "    \"\"\"\n",
    "    Processes waveforms in batches using tf.vectorized_map for parallel single-sample extraction.\n",
    "    No shape errors – works on any TF version.\n",
    "    \"\"\"\n",
    "    def map_fn(waveform):\n",
    "        return extract_single_embedding(waveform)\n",
    "    \n",
    "    # Create dataset and process in parallel\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(waveforms_np)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for batch in dataset:\n",
    "        # vectorized_map processes the batch in parallel (each sample independently)\n",
    "        batch_emb = tf.vectorized_map(map_fn, batch)  # (batch_size, 1, 1024) → squeeze to (batch_size, 1024)\n",
    "        all_embeddings.append(tf.squeeze(batch_emb, axis=1).numpy())\n",
    "    \n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "print(\"Extracting YAMNet embeddings (parallel processing – 1-3 min)...\")\n",
    "train_emb = extract_embeddings_vectorized(X_train)\n",
    "val_emb = extract_embeddings_vectorized(X_val)\n",
    "print(f\"Embeddings ready: train {train_emb.shape}, val {val_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc110619-330c-4d59-8734-19dda6d719b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier on YAMNet embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.5204 - auc: 0.5328 - loss: 0.7086 - val_accuracy: 0.6202 - val_auc: 0.6737 - val_loss: 0.6609 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6019 - auc: 0.5877 - loss: 0.6708 - val_accuracy: 0.6124 - val_auc: 0.6541 - val_loss: 0.6562 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.6330 - auc: 0.6627 - loss: 0.6376 - val_accuracy: 0.6124 - val_auc: 0.6552 - val_loss: 0.6521 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6466 - auc: 0.6901 - loss: 0.6359 - val_accuracy: 0.6357 - val_auc: 0.6899 - val_loss: 0.6396 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6252 - auc: 0.6629 - loss: 0.6400 - val_accuracy: 0.6434 - val_auc: 0.7102 - val_loss: 0.6310 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6408 - auc: 0.6966 - loss: 0.6276 - val_accuracy: 0.6512 - val_auc: 0.7362 - val_loss: 0.6209 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6447 - auc: 0.6885 - loss: 0.6199 - val_accuracy: 0.6744 - val_auc: 0.7658 - val_loss: 0.6048 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6757 - auc: 0.7178 - loss: 0.6175 - val_accuracy: 0.7287 - val_auc: 0.7820 - val_loss: 0.5909 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6835 - auc: 0.7349 - loss: 0.6046 - val_accuracy: 0.7364 - val_auc: 0.7872 - val_loss: 0.5811 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6932 - auc: 0.7130 - loss: 0.6131 - val_accuracy: 0.7597 - val_auc: 0.7965 - val_loss: 0.5717 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7087 - auc: 0.7443 - loss: 0.6069 - val_accuracy: 0.7519 - val_auc: 0.7989 - val_loss: 0.5658 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7087 - auc: 0.7598 - loss: 0.5854 - val_accuracy: 0.7597 - val_auc: 0.8004 - val_loss: 0.5628 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7107 - auc: 0.7523 - loss: 0.5847 - val_accuracy: 0.7829 - val_auc: 0.8021 - val_loss: 0.5514 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7359 - auc: 0.8027 - loss: 0.5569 - val_accuracy: 0.7907 - val_auc: 0.8034 - val_loss: 0.5428 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7184 - auc: 0.7708 - loss: 0.5785 - val_accuracy: 0.8217 - val_auc: 0.8043 - val_loss: 0.5336 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7534 - auc: 0.7882 - loss: 0.5651 - val_accuracy: 0.8140 - val_auc: 0.8082 - val_loss: 0.5320 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7359 - auc: 0.7846 - loss: 0.5660 - val_accuracy: 0.7984 - val_auc: 0.8044 - val_loss: 0.5325 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7379 - auc: 0.7853 - loss: 0.5624 - val_accuracy: 0.8062 - val_auc: 0.8041 - val_loss: 0.5263 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7631 - auc: 0.8147 - loss: 0.5378 - val_accuracy: 0.8217 - val_auc: 0.8064 - val_loss: 0.5142 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7612 - auc: 0.8018 - loss: 0.5530 - val_accuracy: 0.8217 - val_auc: 0.8070 - val_loss: 0.5189 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7825 - auc: 0.8407 - loss: 0.5124 - val_accuracy: 0.8140 - val_auc: 0.8064 - val_loss: 0.5195 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7612 - auc: 0.8173 - loss: 0.5263 - val_accuracy: 0.8295 - val_auc: 0.8101 - val_loss: 0.5006 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7612 - auc: 0.8114 - loss: 0.5463 - val_accuracy: 0.8295 - val_auc: 0.8116 - val_loss: 0.4972 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7612 - auc: 0.8214 - loss: 0.5359 - val_accuracy: 0.8140 - val_auc: 0.8145 - val_loss: 0.4981 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7670 - auc: 0.8333 - loss: 0.5024 - val_accuracy: 0.8140 - val_auc: 0.8149 - val_loss: 0.4903 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7786 - auc: 0.8313 - loss: 0.5117 - val_accuracy: 0.8217 - val_auc: 0.8145 - val_loss: 0.4877 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7670 - auc: 0.8244 - loss: 0.5161 - val_accuracy: 0.8140 - val_auc: 0.8132 - val_loss: 0.4867 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8155 - auc: 0.8516 - loss: 0.4749 - val_accuracy: 0.8140 - val_auc: 0.8147 - val_loss: 0.4907 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8136 - auc: 0.8356 - loss: 0.5014 - val_accuracy: 0.8140 - val_auc: 0.8142 - val_loss: 0.4884 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7961 - auc: 0.8456 - loss: 0.4877 - val_accuracy: 0.8217 - val_auc: 0.8140 - val_loss: 0.4786 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7961 - auc: 0.8476 - loss: 0.4758 - val_accuracy: 0.8217 - val_auc: 0.8164 - val_loss: 0.4785 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8000 - auc: 0.8427 - loss: 0.4899 - val_accuracy: 0.8062 - val_auc: 0.8196 - val_loss: 0.4776 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8097 - auc: 0.8540 - loss: 0.4736 - val_accuracy: 0.7984 - val_auc: 0.8196 - val_loss: 0.4694 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7942 - auc: 0.8500 - loss: 0.4772 - val_accuracy: 0.8062 - val_auc: 0.8198 - val_loss: 0.4698 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8000 - auc: 0.8591 - loss: 0.4711 - val_accuracy: 0.8062 - val_auc: 0.8190 - val_loss: 0.4775 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8136 - auc: 0.8444 - loss: 0.4835 - val_accuracy: 0.8062 - val_auc: 0.8179 - val_loss: 0.4766 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7903 - auc: 0.8597 - loss: 0.4654 - val_accuracy: 0.8062 - val_auc: 0.8190 - val_loss: 0.4753 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8117 - auc: 0.8566 - loss: 0.4706 - val_accuracy: 0.7907 - val_auc: 0.8179 - val_loss: 0.4841 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8252 - auc: 0.8635 - loss: 0.4591 - val_accuracy: 0.7752 - val_auc: 0.8205 - val_loss: 0.4799 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7942 - auc: 0.8605 - loss: 0.4557 - val_accuracy: 0.7984 - val_auc: 0.8202 - val_loss: 0.4761 - learning_rate: 5.0000e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7883 - auc: 0.8586 - loss: 0.4603 - val_accuracy: 0.7984 - val_auc: 0.8217 - val_loss: 0.4735 - learning_rate: 5.0000e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8155 - auc: 0.8721 - loss: 0.4425 - val_accuracy: 0.7907 - val_auc: 0.8231 - val_loss: 0.4740 - learning_rate: 5.0000e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8194 - auc: 0.8655 - loss: 0.4467 - val_accuracy: 0.7984 - val_auc: 0.8242 - val_loss: 0.4702 - learning_rate: 5.0000e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8214 - auc: 0.8710 - loss: 0.4464 - val_accuracy: 0.7984 - val_auc: 0.8247 - val_loss: 0.4682 - learning_rate: 5.0000e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8194 - auc: 0.8572 - loss: 0.4613 - val_accuracy: 0.7984 - val_auc: 0.8258 - val_loss: 0.4660 - learning_rate: 5.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8019 - auc: 0.8526 - loss: 0.4618 - val_accuracy: 0.7984 - val_auc: 0.8288 - val_loss: 0.4627 - learning_rate: 5.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8117 - auc: 0.8789 - loss: 0.4273 - val_accuracy: 0.7984 - val_auc: 0.8296 - val_loss: 0.4603 - learning_rate: 5.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8350 - auc: 0.8714 - loss: 0.4352 - val_accuracy: 0.7984 - val_auc: 0.8289 - val_loss: 0.4619 - learning_rate: 5.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8214 - auc: 0.8757 - loss: 0.4324 - val_accuracy: 0.7907 - val_auc: 0.8300 - val_loss: 0.4621 - learning_rate: 5.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8272 - auc: 0.8569 - loss: 0.4577 - val_accuracy: 0.7907 - val_auc: 0.8341 - val_loss: 0.4530 - learning_rate: 5.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8252 - auc: 0.8689 - loss: 0.4313 - val_accuracy: 0.7907 - val_auc: 0.8360 - val_loss: 0.4486 - learning_rate: 5.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8350 - auc: 0.8801 - loss: 0.4284 - val_accuracy: 0.8062 - val_auc: 0.8383 - val_loss: 0.4465 - learning_rate: 5.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8214 - auc: 0.8741 - loss: 0.4332 - val_accuracy: 0.8140 - val_auc: 0.8387 - val_loss: 0.4456 - learning_rate: 5.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8155 - auc: 0.8741 - loss: 0.4367 - val_accuracy: 0.7907 - val_auc: 0.8388 - val_loss: 0.4468 - learning_rate: 5.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8233 - auc: 0.8758 - loss: 0.4322 - val_accuracy: 0.7829 - val_auc: 0.8383 - val_loss: 0.4499 - learning_rate: 5.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8194 - auc: 0.8787 - loss: 0.4328 - val_accuracy: 0.8062 - val_auc: 0.8398 - val_loss: 0.4465 - learning_rate: 5.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8214 - auc: 0.8836 - loss: 0.4179 - val_accuracy: 0.8062 - val_auc: 0.8409 - val_loss: 0.4459 - learning_rate: 5.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8155 - auc: 0.8828 - loss: 0.4177 - val_accuracy: 0.7907 - val_auc: 0.8417 - val_loss: 0.4472 - learning_rate: 5.0000e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8078 - auc: 0.8864 - loss: 0.4149 - val_accuracy: 0.7984 - val_auc: 0.8423 - val_loss: 0.4492 - learning_rate: 5.0000e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8291 - auc: 0.8936 - loss: 0.3989 - val_accuracy: 0.8062 - val_auc: 0.8452 - val_loss: 0.4411 - learning_rate: 2.5000e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8311 - auc: 0.8761 - loss: 0.4228 - val_accuracy: 0.7907 - val_auc: 0.8493 - val_loss: 0.4350 - learning_rate: 2.5000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8330 - auc: 0.8837 - loss: 0.4211 - val_accuracy: 0.7907 - val_auc: 0.8504 - val_loss: 0.4338 - learning_rate: 2.5000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8388 - auc: 0.8906 - loss: 0.4096 - val_accuracy: 0.7907 - val_auc: 0.8500 - val_loss: 0.4330 - learning_rate: 2.5000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8311 - auc: 0.8840 - loss: 0.4153 - val_accuracy: 0.8140 - val_auc: 0.8542 - val_loss: 0.4277 - learning_rate: 2.5000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8291 - auc: 0.8821 - loss: 0.4209 - val_accuracy: 0.8062 - val_auc: 0.8547 - val_loss: 0.4268 - learning_rate: 2.5000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8233 - auc: 0.8841 - loss: 0.4093 - val_accuracy: 0.7984 - val_auc: 0.8547 - val_loss: 0.4270 - learning_rate: 2.5000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8330 - auc: 0.8852 - loss: 0.4131 - val_accuracy: 0.7984 - val_auc: 0.8553 - val_loss: 0.4265 - learning_rate: 2.5000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8350 - auc: 0.8787 - loss: 0.4158 - val_accuracy: 0.7984 - val_auc: 0.8569 - val_loss: 0.4261 - learning_rate: 2.5000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8252 - auc: 0.8886 - loss: 0.4146 - val_accuracy: 0.8062 - val_auc: 0.8582 - val_loss: 0.4244 - learning_rate: 2.5000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8330 - auc: 0.8859 - loss: 0.4153 - val_accuracy: 0.7984 - val_auc: 0.8585 - val_loss: 0.4242 - learning_rate: 2.5000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8117 - auc: 0.8750 - loss: 0.4282 - val_accuracy: 0.7984 - val_auc: 0.8582 - val_loss: 0.4248 - learning_rate: 2.5000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8311 - auc: 0.8882 - loss: 0.4023 - val_accuracy: 0.7907 - val_auc: 0.8581 - val_loss: 0.4260 - learning_rate: 2.5000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8252 - auc: 0.8921 - loss: 0.4055 - val_accuracy: 0.7984 - val_auc: 0.8601 - val_loss: 0.4245 - learning_rate: 2.5000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8330 - auc: 0.8886 - loss: 0.4028 - val_accuracy: 0.8140 - val_auc: 0.8634 - val_loss: 0.4216 - learning_rate: 2.5000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8350 - auc: 0.8947 - loss: 0.4004 - val_accuracy: 0.8140 - val_auc: 0.8666 - val_loss: 0.4205 - learning_rate: 2.5000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8291 - auc: 0.8970 - loss: 0.3998 - val_accuracy: 0.8140 - val_auc: 0.8653 - val_loss: 0.4196 - learning_rate: 2.5000e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8291 - auc: 0.8841 - loss: 0.4096 - val_accuracy: 0.7907 - val_auc: 0.8642 - val_loss: 0.4201 - learning_rate: 2.5000e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8408 - auc: 0.8919 - loss: 0.4010 - val_accuracy: 0.7907 - val_auc: 0.8642 - val_loss: 0.4199 - learning_rate: 2.5000e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8388 - auc: 0.9054 - loss: 0.3820 - val_accuracy: 0.7907 - val_auc: 0.8640 - val_loss: 0.4204 - learning_rate: 2.5000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8330 - auc: 0.8930 - loss: 0.3937 - val_accuracy: 0.7907 - val_auc: 0.8636 - val_loss: 0.4208 - learning_rate: 2.5000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8233 - auc: 0.8966 - loss: 0.3942 - val_accuracy: 0.7984 - val_auc: 0.8662 - val_loss: 0.4182 - learning_rate: 2.5000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8408 - auc: 0.9099 - loss: 0.3813 - val_accuracy: 0.8062 - val_auc: 0.8683 - val_loss: 0.4165 - learning_rate: 2.5000e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8447 - auc: 0.9011 - loss: 0.3848 - val_accuracy: 0.8062 - val_auc: 0.8690 - val_loss: 0.4174 - learning_rate: 2.5000e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8369 - auc: 0.8976 - loss: 0.3969 - val_accuracy: 0.8062 - val_auc: 0.8693 - val_loss: 0.4168 - learning_rate: 2.5000e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8388 - auc: 0.9022 - loss: 0.3862 - val_accuracy: 0.8062 - val_auc: 0.8696 - val_loss: 0.4161 - learning_rate: 2.5000e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8311 - auc: 0.8892 - loss: 0.4026 - val_accuracy: 0.7907 - val_auc: 0.8691 - val_loss: 0.4164 - learning_rate: 2.5000e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8524 - auc: 0.9025 - loss: 0.3803 - val_accuracy: 0.7907 - val_auc: 0.8689 - val_loss: 0.4162 - learning_rate: 2.5000e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8466 - auc: 0.9037 - loss: 0.3827 - val_accuracy: 0.7907 - val_auc: 0.8682 - val_loss: 0.4161 - learning_rate: 2.5000e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8330 - auc: 0.9080 - loss: 0.3804 - val_accuracy: 0.7907 - val_auc: 0.8669 - val_loss: 0.4177 - learning_rate: 2.5000e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8408 - auc: 0.8940 - loss: 0.3950 - val_accuracy: 0.7907 - val_auc: 0.8683 - val_loss: 0.4169 - learning_rate: 2.5000e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8311 - auc: 0.8902 - loss: 0.3980 - val_accuracy: 0.8062 - val_auc: 0.8689 - val_loss: 0.4160 - learning_rate: 2.5000e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8447 - auc: 0.9060 - loss: 0.3783 - val_accuracy: 0.8062 - val_auc: 0.8691 - val_loss: 0.4157 - learning_rate: 2.5000e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8427 - auc: 0.8947 - loss: 0.3917 - val_accuracy: 0.8062 - val_auc: 0.8733 - val_loss: 0.4139 - learning_rate: 2.5000e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8485 - auc: 0.9002 - loss: 0.3867 - val_accuracy: 0.8062 - val_auc: 0.8744 - val_loss: 0.4116 - learning_rate: 2.5000e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8427 - auc: 0.9024 - loss: 0.3839 - val_accuracy: 0.8140 - val_auc: 0.8784 - val_loss: 0.4085 - learning_rate: 2.5000e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8408 - auc: 0.8960 - loss: 0.3914 - val_accuracy: 0.8062 - val_auc: 0.8778 - val_loss: 0.4076 - learning_rate: 2.5000e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8505 - auc: 0.9027 - loss: 0.3808 - val_accuracy: 0.8062 - val_auc: 0.8768 - val_loss: 0.4077 - learning_rate: 2.5000e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8330 - auc: 0.8938 - loss: 0.3906 - val_accuracy: 0.7907 - val_auc: 0.8767 - val_loss: 0.4076 - learning_rate: 2.5000e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8427 - auc: 0.9095 - loss: 0.3740 - val_accuracy: 0.7907 - val_auc: 0.8773 - val_loss: 0.4075 - learning_rate: 2.5000e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8330 - auc: 0.9008 - loss: 0.3891 - val_accuracy: 0.7907 - val_auc: 0.8778 - val_loss: 0.4063 - learning_rate: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- 5. Train classification head -------------------------------\n",
    "input_layer = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(input_layer)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # PD probability\n",
    "\n",
    "classifier = tf.keras.Model(input_layer, output)\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "print(\"Training classifier on YAMNet embeddings...\")\n",
    "history = classifier.fit(\n",
    "    train_emb, y_train,\n",
    "    validation_data=(val_emb, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=12, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=6, factor=0.5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe009f20-295a-45eb-af6e-4b3ef360b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.791 | AUC: 0.878\n",
      "Classifier head saved as 'yamnet_pd_classifier_head.keras'\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- 6. Final evaluation -------------------------------\n",
    "val_loss, val_acc, val_auc = classifier.evaluate(val_emb, y_val, verbose=0)\n",
    "print(f\"\\nValidation Accuracy: {val_acc:.3f} | AUC: {val_auc:.3f}\")\n",
    "\n",
    "# ------------------------------- 7. Save the classifier head (optional) -------------------------------\n",
    "classifier.save(\"yamnet_pd_classifier_head.keras\")\n",
    "print(\"Classifier head saved as 'yamnet_pd_classifier_head.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f679dd3-d2f4-40f8-84d4-9e36175676bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier on YAMNet embeddings (smart early stopping + best model saving)...\n",
      "Epoch 1/200\n",
      "\u001b[1m6/9\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5303 - auc: 0.5273 - loss: 0.7174\n",
      "Epoch 1: val_auc improved from None to 0.74902, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.5612 - auc: 0.5327 - loss: 0.7082 - val_accuracy: 0.7829 - val_auc: 0.7490 - val_loss: 0.6645 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5359 - auc: 0.5409 - loss: 0.6918\n",
      "Epoch 2: val_auc did not improve from 0.74902\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5689 - auc: 0.5796 - loss: 0.6747 - val_accuracy: 0.6589 - val_auc: 0.7420 - val_loss: 0.6447 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6016 - auc: 0.6051 - loss: 0.6701\n",
      "Epoch 3: val_auc did not improve from 0.74902\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6117 - auc: 0.6004 - loss: 0.6703 - val_accuracy: 0.6512 - val_auc: 0.7363 - val_loss: 0.6355 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6430 - auc: 0.7194 - loss: 0.6250\n",
      "Epoch 4: val_auc did not improve from 0.74902\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6369 - auc: 0.6825 - loss: 0.6372 - val_accuracy: 0.6512 - val_auc: 0.7433 - val_loss: 0.6260 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6532 - auc: 0.7060 - loss: 0.6334\n",
      "Epoch 5: val_auc improved from 0.74902 to 0.75220, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6621 - auc: 0.6755 - loss: 0.6384 - val_accuracy: 0.6512 - val_auc: 0.7522 - val_loss: 0.6164 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6277 - auc: 0.6943 - loss: 0.6318\n",
      "Epoch 6: val_auc improved from 0.75220 to 0.76908, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6505 - auc: 0.7013 - loss: 0.6221 - val_accuracy: 0.6667 - val_auc: 0.7691 - val_loss: 0.6068 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6903 - auc: 0.6746 - loss: 0.6229\n",
      "Epoch 7: val_auc improved from 0.76908 to 0.78694, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6835 - auc: 0.6811 - loss: 0.6236 - val_accuracy: 0.7054 - val_auc: 0.7869 - val_loss: 0.5947 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6684 - auc: 0.7127 - loss: 0.6185\n",
      "Epoch 8: val_auc improved from 0.78694 to 0.79525, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6660 - auc: 0.7111 - loss: 0.6188 - val_accuracy: 0.7364 - val_auc: 0.7953 - val_loss: 0.5875 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6680 - auc: 0.7442 - loss: 0.5938\n",
      "Epoch 9: val_auc did not improve from 0.79525\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6699 - auc: 0.7371 - loss: 0.6070 - val_accuracy: 0.7364 - val_auc: 0.7949 - val_loss: 0.5838 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6347 - auc: 0.7104 - loss: 0.6210\n",
      "Epoch 10: val_auc improved from 0.79525 to 0.80479, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6893 - auc: 0.7414 - loss: 0.5953 - val_accuracy: 0.7829 - val_auc: 0.8048 - val_loss: 0.5730 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6842 - auc: 0.7334 - loss: 0.6103\n",
      "Epoch 11: val_auc improved from 0.80479 to 0.81152, saving model to best_pd_yamnet_classifier.keras\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6874 - auc: 0.7439 - loss: 0.6004 - val_accuracy: 0.8140 - val_auc: 0.8115 - val_loss: 0.5620 - learning_rate: 1.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7011 - auc: 0.7313 - loss: 0.5950\n",
      "Epoch 12: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7029 - auc: 0.7360 - loss: 0.5948 - val_accuracy: 0.8140 - val_auc: 0.8092 - val_loss: 0.5551 - learning_rate: 1.0000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7085 - auc: 0.8001 - loss: 0.5677 \n",
      "Epoch 13: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7029 - auc: 0.7774 - loss: 0.5789 - val_accuracy: 0.7984 - val_auc: 0.8068 - val_loss: 0.5484 - learning_rate: 1.0000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7087 - auc: 0.7611 - loss: 0.5696\n",
      "Epoch 14: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7010 - auc: 0.7450 - loss: 0.5865 - val_accuracy: 0.7829 - val_auc: 0.8047 - val_loss: 0.5446 - learning_rate: 1.0000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7260 - auc: 0.7640 - loss: 0.5754\n",
      "Epoch 15: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7379 - auc: 0.7740 - loss: 0.5803 - val_accuracy: 0.8372 - val_auc: 0.8098 - val_loss: 0.5332 - learning_rate: 1.0000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m6/9\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7262 - auc: 0.7947 - loss: 0.5631\n",
      "Epoch 16: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7165 - auc: 0.7889 - loss: 0.5710 - val_accuracy: 0.8062 - val_auc: 0.8061 - val_loss: 0.5355 - learning_rate: 1.0000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7534 - auc: 0.7831 - loss: 0.5710\n",
      "Epoch 17: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7165 - auc: 0.7801 - loss: 0.5758 - val_accuracy: 0.8062 - val_auc: 0.8037 - val_loss: 0.5347 - learning_rate: 1.0000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7515 - auc: 0.8184 - loss: 0.5310\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 18: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7476 - auc: 0.8024 - loss: 0.5530 - val_accuracy: 0.8217 - val_auc: 0.8058 - val_loss: 0.5275 - learning_rate: 1.0000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7465 - auc: 0.7635 - loss: 0.5796\n",
      "Epoch 19: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7573 - auc: 0.7852 - loss: 0.5654 - val_accuracy: 0.8217 - val_auc: 0.8069 - val_loss: 0.5251 - learning_rate: 5.0000e-05\n",
      "Epoch 20/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7272 - auc: 0.7714 - loss: 0.5800\n",
      "Epoch 20: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7456 - auc: 0.8039 - loss: 0.5604 - val_accuracy: 0.8140 - val_auc: 0.8069 - val_loss: 0.5232 - learning_rate: 5.0000e-05\n",
      "Epoch 21/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7773 - auc: 0.8096 - loss: 0.5334\n",
      "Epoch 21: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7922 - auc: 0.8195 - loss: 0.5361 - val_accuracy: 0.8140 - val_auc: 0.8094 - val_loss: 0.5189 - learning_rate: 5.0000e-05\n",
      "Epoch 22/200\n",
      "\u001b[1m7/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7757 - auc: 0.8023 - loss: 0.5334\n",
      "Epoch 22: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7786 - auc: 0.8288 - loss: 0.5240 - val_accuracy: 0.8062 - val_auc: 0.8105 - val_loss: 0.5136 - learning_rate: 5.0000e-05\n",
      "Epoch 23/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7457 - auc: 0.8010 - loss: 0.5349\n",
      "Epoch 23: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7456 - auc: 0.7892 - loss: 0.5510 - val_accuracy: 0.8140 - val_auc: 0.8115 - val_loss: 0.5108 - learning_rate: 5.0000e-05\n",
      "Epoch 24/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7442 - auc: 0.7988 - loss: 0.5378\n",
      "Epoch 24: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7534 - auc: 0.8100 - loss: 0.5389 - val_accuracy: 0.8217 - val_auc: 0.8108 - val_loss: 0.5089 - learning_rate: 5.0000e-05\n",
      "Epoch 25/200\n",
      "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7605 - auc: 0.8046 - loss: 0.5485 \n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 25: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7650 - auc: 0.8218 - loss: 0.5278 - val_accuracy: 0.8217 - val_auc: 0.8102 - val_loss: 0.5084 - learning_rate: 5.0000e-05\n",
      "Epoch 26/200\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7582 - auc: 0.8311 - loss: 0.5262\n",
      "Epoch 26: val_auc did not improve from 0.81152\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7786 - auc: 0.8306 - loss: 0.5192 - val_accuracy: 0.8217 - val_auc: 0.8098 - val_loss: 0.5081 - learning_rate: 2.5000e-05\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# ------------------------------- 5. Train classification head (with smart callbacks) -------------------------------\n",
    "input_layer = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(input_layer)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='pd_probability')(x)\n",
    "\n",
    "classifier = tf.keras.Model(input_layer, output)\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"Training classifier on YAMNet embeddings (smart early stopping + best model saving)...\")\n",
    "\n",
    "# CALLBACKS – This is the gold standard for medical/binary tasks\n",
    "callbacks = [\n",
    "    # 1. Stop training when val_auc stops improving (best metric for imbalanced PD data)\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',          # Focus on AUC, not accuracy (more robust for PD detection)\n",
    "        mode='max',                 # We want to maximize AUC\n",
    "        patience=15,                # Wait 15 epochs after peak\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # 2. Reduce learning rate when stuck\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='accuracy',\n",
    "        mode='max',\n",
    "        factor=0.5,                 # Halve LR\n",
    "        patience=7,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # 3. Save ONLY the best model (by AUC)\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_pd_yamnet_classifier.keras',  # or .h5 if older TF\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = classifier.fit(\n",
    "    train_emb, y_train,\n",
    "    validation_data=(val_emb, y_val),\n",
    "    epochs=200,                    # High number – callbacks will stop early\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16731e1-791c-43f4-b0af-bb8178052b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING FINISHED – Loading best model and evaluating...\n",
      "BEST VALIDATION → Accuracy: 0.8140 | AUC: 0.8115\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- Final evaluation on best model -------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING FINISHED – Loading best model and evaluating...\")\n",
    "best_model = tf.keras.models.load_model('best_pd_yamnet_classifier.keras')\n",
    "\n",
    "val_loss, val_acc, val_auc = best_model.evaluate(val_emb, y_val, verbose=0)\n",
    "print(f\"BEST VALIDATION → Accuracy: {val_acc:.4f} | AUC: {val_auc:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee26bb98-1120-400e-b688-b4a52148e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST VALIDATION → Accuracy: 0.7907 | AUC: 0.8778\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_auc = classifier.evaluate(val_emb, y_val, verbose=0)\n",
    "print(f\"BEST VALIDATION → Accuracy: {val_acc:.4f} | AUC: {val_auc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "780762a0-0a97-4620-9201-acb3e9d32412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline succesfully done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load YAMNet as KerasLayer (trainable=False)\n",
    "yamnet_layer = hub.KerasLayer(\"https://tfhub.dev/google/yamnet/1\", trainable=False)\n",
    "\n",
    "# 2. Load your trained classifier head\n",
    "classifier_head = tf.keras.models.load_model(\"best_pd_yamnet_classifier.keras\")\n",
    "# or: classifier_head = tf.keras.models.load_model(\"yamnet_pd_classifier_final.keras\")\n",
    "\n",
    "# 3. Create the end-to-end model with tf.map_fn (this bypasses the batching bug)\n",
    "class PDYamNetEndToEnd(tf.keras.Model):\n",
    "    def __init__(self, yamnet, head):\n",
    "        super().__init__()\n",
    "        self.yamnet = yamnet\n",
    "        self.head = head\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs: (batch, 80000) or (80000,)\n",
    "        # Force single-sample processing → no more shape casting error\n",
    "        def process_one(waveform):\n",
    "            waveform = tf.expand_dims(waveform, 0)                    # (1, 80000)\n",
    "            waveform = tf.cast(waveform, tf.float32) / 32768.0        # → [-1, 1]\n",
    "            _, embeddings, _ = self.yamnet(waveform)                  # (1, time, 1024)\n",
    "            pooled = tf.reduce_mean(embeddings, axis=1)               # (1, 1024)\n",
    "            return self.head(pooled)                                  # (1, 1)\n",
    "\n",
    "        # This handles both batched and single input\n",
    "        result = tf.map_fn(\n",
    "            process_one,\n",
    "            inputs,\n",
    "            fn_output_signature=tf.float32\n",
    "        )\n",
    "        return tf.squeeze(result, axis=-1)  # (batch,) or scalar\n",
    "\n",
    "# Build the model\n",
    "model = PDYamNetEndToEnd(yamnet_layer, classifier_head)\n",
    "\n",
    "# Build with correct shape\n",
    "model.build(input_shape=(None, 80000))\n",
    "print('Pipeline succesfully done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e34901-67f7-47d3-a0d0-3f21c5813a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IT — this will now work!\n",
    "dummy = np.random.randn(1, 80000).astype(np.float32) * 0.1\n",
    "pred = model(dummy, training=False)\n",
    "print(\"Success! Test prediction:\", pred.numpy()[0])   # → something like 0.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea22935-e4ee-48f3-af8b-0601d6590398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert to TFLite (fully quantized for maximum speed & smallest size)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(full_model)\n",
    "\n",
    "\n",
    "# This quantization gives ~4× speed-up and 4× smaller file with almost zero accuracy drop\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]          # Float16 quantization (best for mobile)\n",
    "converter.inference_input_type = tf.float32                   # You can also use tf.int16 if you prefer\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 6. Save the final model\n",
    "with open(\"pd_yamnet_end2end.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"SUCCESS! → pd_yamnet_end2end.tflite created\")\n",
    "print(f\"Size: {len(tflite_model)/1024/1024:.1f} MB\")\n",
    "print(\"Ready for Flutter – just drop it into assets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4601ca2-3b32-4e87-98e3-80f9caa3e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Felix\\AppData\\Local\\Temp\\tmpjmeflhxi\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Felix\\AppData\\Local\\Temp\\tmpjmeflhxi\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Felix\\AppData\\Local\\Temp\\tmpjmeflhxi'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1024), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1525379669008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379670160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379669776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379670736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379670352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379671312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379670928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525379671888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Done! Two files created:\n",
      "   - yamnet.tflite          (15 MB)  ← Google's official\n",
      "   - pd_classifier_only.tflite (∼2 MB) ← Your PD detector\n"
     ]
    }
   ],
   "source": [
    "# 1. Download the official pre-converted YAMNet TFLite (already quantized & perfect)\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://storage.googleapis.com/audioset/yamnet.tflite\",\n",
    "    \"yamnet.tflite\"\n",
    ")\n",
    "\n",
    "# 2. Save ONLY your classifier head as TFLite (this always works)\n",
    "classifier_head = tf.keras.models.load_model(\"best_pd_yamnet_classifier.keras\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(classifier_head)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "pd_classifier_tflite = converter.convert()\n",
    "\n",
    "with open(\"pd_classifier_only.tflite\", \"wb\") as f:\n",
    "    f.write(pd_classifier_tflite)\n",
    "\n",
    "print(\"Done! Two files created:\")\n",
    "print(\"   - yamnet.tflite          (15 MB)  ← Google's official\")\n",
    "print(\"   - pd_classifier_only.tflite (∼2 MB) ← Your PD detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "408717c4-5c9c-4c48-b741-29fc112fe0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All outputs:\n",
      "  Output 0: shape (1, 521) → 521 elements\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Embedding not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m         t \u001b[38;5;241m=\u001b[39m yamnet\u001b[38;5;241m.\u001b[39mget_tensor(od[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Output \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# === NOW TEST WITH REAL AUDIO ===\u001b[39;00m\n\u001b[0;32m     36\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_5sec/PD/ID02_pd_2_0_0.wav_seg0_0000.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# ← CHANGE THIS\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Embedding not found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "\n",
    "Interpreter = tf.lite.Interpreter\n",
    "\n",
    "# Load models\n",
    "yamnet = Interpreter(model_path=\"yamnet.tflite\")\n",
    "pd     = Interpreter(model_path=\"pd_classifier_only.tflite\")\n",
    "yamnet.allocate_tensors()\n",
    "pd.allocate_tensors()\n",
    "\n",
    "# === CRITICAL: Run once with dummy data to materialize real output shapes ===\n",
    "dummy_input = np.zeros(15600, dtype=np.float32)\n",
    "yamnet.set_tensor(yamnet.get_input_details()[0]['index'], dummy_input)\n",
    "yamnet.invoke()                                   # ← THIS LINE WAS MISSING\n",
    "\n",
    "# Now detect which output has exactly 1024 elements (the embedding)\n",
    "output_details = yamnet.get_output_details()\n",
    "embedding_idx = None\n",
    "for i, od in enumerate(output_details):\n",
    "    tensor = yamnet.get_tensor(od['index'])\n",
    "    if tensor.size == 1024:                       # ← this is the key\n",
    "        embedding_idx = i\n",
    "        print(f\"Embedding found at output {i} → shape {tensor.shape}\")\n",
    "        break\n",
    "\n",
    "if embedding_idx is None:\n",
    "    print(\"All outputs:\")\n",
    "    for i, od in enumerate(output_details):\n",
    "        t = yamnet.get_tensor(od['index'])\n",
    "        print(f\"  Output {i}: shape {t.shape} → {t.size} elements\")\n",
    "    raise RuntimeError(\"Embedding not found\")\n",
    "\n",
    "# === NOW TEST WITH REAL AUDIO ===\n",
    "audio_path = \"dataset_5sec/PD/ID02_pd_2_0_0.wav_seg0_0000.wav\"   # ← CHANGE THIS\n",
    "# audio_path = \"dataset_5sec/HC/your_hc_file.wav\"\n",
    "\n",
    "waveform, _ = librosa.load(audio_path, sr=16000, mono=True)\n",
    "if len(waveform) > 15600:\n",
    "    waveform = waveform[:15600]\n",
    "else:\n",
    "    waveform = np.pad(waveform, (0, 15600 - len(waveform)))\n",
    "waveform = waveform.astype(np.float32)\n",
    "\n",
    "# Run YAMNet\n",
    "yamnet.set_tensor(yamnet.get_input_details()[0]['index'], waveform)\n",
    "yamnet.invoke()\n",
    "\n",
    "# Get embedding\n",
    "embedding = yamnet.get_tensor(output_details[embedding_idx]['index'])  # (1,1024) or (1024,)\n",
    "\n",
    "# Make sure it's (1, 1024)\n",
    "if embedding.shape == (1024,):\n",
    "    embedding = embedding.reshape(1, 1024)\n",
    "\n",
    "# Run your PD classifier\n",
    "pd.set_tensor(pd.get_input_details()[0]['index'], embedding)\n",
    "pd.invoke()\n",
    "prob = pd.get_tensor(pd.get_output_details()[0]['index'])[0][0]\n",
    "\n",
    "# Result\n",
    "label = \"Parkinson’s\" if prob > 0.5 else \"Healthy Control\"\n",
    "conf = max(prob, 1 - prob)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FILE           : {audio_path.split('/')[-1]}\")\n",
    "print(f\"PD PROBABILITY : {prob:.4f}\")\n",
    "print(f\"PREDICTION     : {label}\")\n",
    "print(f\"CONFIDENCE     : {conf:.1%}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce29e97-a108-405f-8f45-f3210201824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b86bea81-ddec-4c69-ac82-1d114fa1d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing on Dummy Audio: **PARK_DATA/PD_segments/ID20_pd_3_0_1.wav_seg1.wav**\n",
      "Raw Prediction (Probabilities): [0.6760113]\n",
      "Predicted Class: **Parkinson_Negative**\n",
      "Confidence: **0.6760**\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Configuration (Required by YAMNet) ---\n",
    "# YAMNet requires audio to be resampled to 16kHz and mono\n",
    "SAMPLE_RATE = 16000\n",
    "EMBEDDING_SIZE = 1024 # The dimension of YAMNet's output embedding\n",
    "\n",
    "# --- 2. Load the Feature Extractor Model ---\n",
    "# This is the base YAMNet model which extracts the 1024-dim features\n",
    "# The handle points to the latest YAMNet model on TF Hub\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(YAMNET_MODEL_HANDLE)\n",
    "\n",
    "# --- 3. Function to Process Audio and Extract Embeddings ---\n",
    "def get_embeddings(audio_path):\n",
    "    \"\"\"\n",
    "    Loads audio, extracts YAMNet embeddings, and returns the average embedding.\n",
    "    \"\"\"\n",
    "    # Load audio, resample to 16kHz, mono, and normalize to [-1.0, 1.0]\n",
    "    waveform, _ = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
    "    \n",
    "    # Extract raw embeddings using the YAMNet model\n",
    "    # The output is (scores, embeddings, spectrogram)\n",
    "    # The embeddings shape will be (N, 1024), where N is the number of frames\n",
    "    _, embeddings, _ = yamnet_model(waveform)\n",
    "    \n",
    "    # Average the embeddings across all frames (N) to get a single (1024,) vector.\n",
    "    # This matches the input shape expected by your custom classifier\n",
    "    # trained on averaged embeddings.\n",
    "    avg_embedding = np.mean(embeddings, axis=0, keepdims=True) # Shape (1, 1024)\n",
    "    return avg_embedding\n",
    "\n",
    "# --- 4. Testing on Dummy Audio ---\n",
    "\n",
    "# **REPLACE THIS WITH YOUR AUDIO FILE PATH**\n",
    "dummy_audio_file = 'PARK_DATA/PD_segments/ID20_pd_3_0_1.wav_seg1.wav' \n",
    "# **REPLACE THIS WITH YOUR ACTUAL CLASS NAMES**\n",
    "# This list is necessary to interpret the prediction index\n",
    "class_names = ['Parkinson_Negative', 'Parkinson_Positive'] \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Testing on Dummy Audio: **{dummy_audio_file}**\")\n",
    "\n",
    "# Get the averaged YAMNet embedding\n",
    "test_embedding = get_embeddings(dummy_audio_file)\n",
    "\n",
    "# Load your custom classifier model\n",
    "best_model = tf.keras.models.load_model('best_pd_yamnet_classifier.keras')\n",
    "\n",
    "# Make the prediction\n",
    " def predict(folder):\n",
    "    for files in# The input to `predict` is the embedding with shape (1, 1024)\n",
    "    predictions = best_model.predict(test_embedding, verbose=0)\n",
    "    \n",
    "    # Interpret the Result\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_probability = predictions[0][predicted_class_index]\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "    \n",
    "    print(f\"Raw Prediction (Probabilities): {predictions[0]}\")\n",
    "    print(f\"Predicted Class: **{predicted_class_name}**\")\n",
    "    print(f\"Confidence: **{predicted_probability:.4f}**\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5a8a754-fa8f-4861-a943-ba917b7a3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os # For folder operations\n",
    "import pandas as pd # For presenting results nicely\n",
    "\n",
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67124222-717a-4402-add1-b0b1dae14f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YAMNet Feature Extractor...\n",
      "Loading Custom Classifier...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 1. Configuration (Required by YAMNet)\n",
    "SAMPLE_RATE = 16000\n",
    "EMBEDDING_SIZE = 1024 \n",
    "# Set the threshold for classifying the positive class (often 0.5)\n",
    "CLASSIFICATION_THRESHOLD = 0.5 \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 2. Load the Feature Extractor Model & Custom Classifier\n",
    "print(\"Loading YAMNet Feature Extractor...\")\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(YAMNET_MODEL_HANDLE)\n",
    "\n",
    "print(\"Loading Custom Classifier...\")\n",
    "# Load your custom classifier model once\n",
    "best_model = tf.keras.models.load_model('best_pd_yamnet_classifier.keras')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 3. Function to Process Audio and Extract Embeddings\n",
    "def get_embeddings(audio_path):\n",
    "    \"\"\"\n",
    "    Loads audio, extracts YAMNet embeddings, and returns the average embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio, resample to 16kHz, mono\n",
    "        waveform, _ = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {audio_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract raw embeddings\n",
    "    _, embeddings, _ = yamnet_model(waveform)\n",
    "    \n",
    "    # Average the embeddings across all frames to get a single (1, 1024) vector.\n",
    "    avg_embedding = np.mean(embeddings, axis=0, keepdims=True)\n",
    "    return avg_embedding\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 4. Function to Predict for all Files in a Folder\n",
    "def predict_folder_old(folder_path, class_names):\n",
    "    \"\"\"\n",
    "    Iterates through all .wav files in a folder, extracts features, and predicts\n",
    "    using a model trained with binary_crossentropy (sigmoid output).\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"\\n🚫 Error: Folder not found at {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✨ Starting Prediction for folder: **{folder_path}**\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(('.wav', '.mp3', '.flac')):\n",
    "            audio_path = os.path.join(folder_path, filename)\n",
    "            print(f\"-> Processing {filename}...\")\n",
    "\n",
    "            test_embedding = get_embeddings(audio_path)\n",
    "            \n",
    "            if test_embedding is not None:\n",
    "                # Make the prediction\n",
    "                # Output shape is (1, 1) or (1,), representing P(Positive Class)\n",
    "                predictions = best_model.predict(test_embedding, verbose=0)\n",
    "                \n",
    "                # --- ADJUSTMENT FOR BINARY CROSSENTROPY / SIGMOID ---\n",
    "                \n",
    "                # The single prediction value is the probability of the positive class\n",
    "                positive_prob = predictions[0][0] \n",
    "                \n",
    "                # Classify based on the threshold (e.g., 0.5)\n",
    "                if positive_prob >= CLASSIFICATION_THRESHOLD:\n",
    "                    predicted_class_name = class_names[1] # The positive class\n",
    "                    confidence = positive_prob\n",
    "                else:\n",
    "                    predicted_class_name = class_names[0] # The negative class\n",
    "                    # Confidence in the negative class is 1 - positive_prob\n",
    "                    confidence = 1.0 - positive_prob\n",
    "                \n",
    "                # ------------------------------------------------------\n",
    "                \n",
    "                # Store the result\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'Predicted_Class': predicted_class_name,\n",
    "                    'Confidence': f\"{confidence:.4f}\",\n",
    "                    'Raw_Positive_Probability': f\"{positive_prob:.4f}\"\n",
    "                })\n",
    "            \n",
    "    # Display results in a nice table\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"✅ Prediction Summary:\")\n",
    "        # Display the filename, predicted class, and the raw positive probability\n",
    "        print(df[['Filename', 'Predicted_Class', 'Raw_Positive_Probability']].to_markdown(index=False))\n",
    "        print(f\"\\nNote: Prediction uses a threshold of **{CLASSIFICATION_THRESHOLD}** on Raw Positive Probability.\")\n",
    "        print(\"-\"*80)\n",
    "    else:\n",
    "        print(\"\\n⚠️ No audio files processed.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e424cd64-ee98-44fe-bce3-d47ce7bf5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✨ Starting Prediction for folder: **PARK_DATA - Copy/PD_segments**\n",
      "================================================================================\n",
      "-> Processing ID02_pd_2_0_0.wav_seg1.wav...\n",
      "-> Processing ID04_pd_2_0_1.wav_seg0.wav...\n",
      "-> Processing ID06_pd_3_1_1.wav_seg0.wav...\n",
      "-> Processing ID07_pd_2_0_0.wav_seg0.wav...\n",
      "-> Processing ID13_pd_3_2_2.wav_seg0.wav...\n",
      "-> Processing ID16_pd_2_0_0.wav_seg0.wav...\n",
      "-> Processing ID18_pd_4_3_3.wav_seg0.wav...\n",
      "-> Processing ID20_pd_3_0_1.wav_seg1.wav...\n",
      "-> Processing ID24_pd_2_0_0.wav_seg1.wav...\n",
      "-> Processing ID27_pd_4_1_1.wav_seg0.wav...\n",
      "-> Processing ID29_pd_3_1_2.wav_seg2.wav...\n",
      "-> Processing ID30_pd_2_1_1.wav_seg1.wav...\n",
      "-> Processing ID32_pd_3_1_1.wav_seg0.wav...\n",
      "-> Processing ID33_pd_3_2_2.wav_seg0.wav...\n",
      "-> Processing ID34_pd_2_0_0.wav_seg1.wav...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Prediction Summary:\n",
      "| Filename                   | Predicted_Class    |   Raw_Positive_Probability |\n",
      "|:---------------------------|:-------------------|---------------------------:|\n",
      "| ID02_pd_2_0_0.wav_seg1.wav | Parkinson_Positive |                     0.6459 |\n",
      "| ID04_pd_2_0_1.wav_seg0.wav | Parkinson_Negative |                     0.4737 |\n",
      "| ID06_pd_3_1_1.wav_seg0.wav | Parkinson_Positive |                     0.5144 |\n",
      "| ID07_pd_2_0_0.wav_seg0.wav | Parkinson_Positive |                     0.6442 |\n",
      "| ID13_pd_3_2_2.wav_seg0.wav | Parkinson_Positive |                     0.5474 |\n",
      "| ID16_pd_2_0_0.wav_seg0.wav | Parkinson_Negative |                     0.4553 |\n",
      "| ID18_pd_4_3_3.wav_seg0.wav | Parkinson_Positive |                     0.9651 |\n",
      "| ID20_pd_3_0_1.wav_seg1.wav | Parkinson_Positive |                     0.676  |\n",
      "| ID24_pd_2_0_0.wav_seg1.wav | Parkinson_Positive |                     0.5057 |\n",
      "| ID27_pd_4_1_1.wav_seg0.wav | Parkinson_Negative |                     0.4805 |\n",
      "| ID29_pd_3_1_2.wav_seg2.wav | Parkinson_Positive |                     0.7269 |\n",
      "| ID30_pd_2_1_1.wav_seg1.wav | Parkinson_Negative |                     0.3099 |\n",
      "| ID32_pd_3_1_1.wav_seg0.wav | Parkinson_Positive |                     0.531  |\n",
      "| ID33_pd_3_2_2.wav_seg0.wav | Parkinson_Positive |                     0.7674 |\n",
      "| ID34_pd_2_0_0.wav_seg1.wav | Parkinson_Negative |                     0.3071 |\n",
      "\n",
      "Note: Prediction uses a threshold of **0.5** on Raw Positive Probability.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## 5. Execution Block\n",
    "# **DEFINE THE FOLDER YOU WANT TO TEST**\n",
    "TEST_FOLDER = 'PARK_DATA - Copy/PD_segments' \n",
    "\n",
    "# **DEFINE YOUR ACTUAL CLASS NAMES**\n",
    "# Class Names MUST be ordered as [Negative Class, Positive Class]\n",
    "CLASS_NAMES = ['Parkinson_Negative', 'Parkinson_Positive'] \n",
    "\n",
    "predict_folder_old(TEST_FOLDER, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6d9ea-1c36-4ac1-bd90-5ce8e539d9b0",
   "metadata": {},
   "source": [
    "### Maximum voting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66855278-840c-47eb-8a9f-7737ca739fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YAMNet Feature Extractor...\n",
      "Loading Custom Classifier...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter # For max voting\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 1. Configuration & Global Variables\n",
    "# YAMNet requirements\n",
    "SAMPLE_RATE = 16000\n",
    "EMBEDDING_SIZE = 1024 \n",
    "# The duration of the segment the model was trained on\n",
    "SEGMENT_DURATION_SECONDS = 5 \n",
    "# Set the threshold for classifying the positive class (often 0.5)\n",
    "CLASSIFICATION_THRESHOLD = 0.5 \n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 2. Load the Feature Extractor Model & Custom Classifier\n",
    "print(\"Loading YAMNet Feature Extractor...\")\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(YAMNET_MODEL_HANDLE)\n",
    "\n",
    "print(\"Loading Custom Classifier...\")\n",
    "# Load your custom classifier model once\n",
    "best_model = tf.keras.models.load_model('best_pd_yamnet_classifier.keras')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 3. Core Function: Embeddings Extraction\n",
    "def extract_single_embedding(waveform_segment):\n",
    "    \"\"\"\n",
    "    Extracts the averaged YAMNet embedding from a pre-loaded waveform segment.\n",
    "    Input: (N,) numpy array of audio samples\n",
    "    Output: (1, 1024) numpy array\n",
    "    \"\"\"\n",
    "    # Extract raw embeddings using the YAMNet model\n",
    "    # YAMNet handles the internal 0.96s frames\n",
    "    _, embeddings, _ = yamnet_model(waveform_segment)\n",
    "    \n",
    "    # Average the embeddings across all frames to get a single (1, 1024) vector.\n",
    "    avg_embedding = np.mean(embeddings, axis=0, keepdims=True)\n",
    "    return avg_embedding\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 4. Main Function: Segmentation, Prediction, and Voting\n",
    "def segment_and_predict(audio_path, class_names):\n",
    "    \"\"\"\n",
    "    Loads a long audio file, splits it into 5-second segments,\n",
    "    predicts the class for each segment, and uses Max Voting for the final result.\n",
    "    \"\"\"\n",
    "    # --- Step 4.1: Load the full audio file ---\n",
    "    try:\n",
    "        # Load audio, resample to 16kHz, mono\n",
    "        full_waveform, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {audio_path}: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    total_samples = len(full_waveform)\n",
    "    segment_samples = SEGMENT_DURATION_SECONDS * sr\n",
    "    \n",
    "    if total_samples < segment_samples:\n",
    "         print(f\"Warning: File too short ({total_samples/sr:.2f}s). Skipping segmentation.\")\n",
    "         # If too short, just process it as a single segment\n",
    "         num_segments = 1\n",
    "    else:\n",
    "         # Calculate the number of non-overlapping 5-second segments\n",
    "         num_segments = total_samples // segment_samples\n",
    "         \n",
    "    segment_predictions = []\n",
    "    \n",
    "    # --- Step 4.2: Iterate through segments ---\n",
    "    for i in range(num_segments):\n",
    "        start_sample = i * segment_samples\n",
    "        end_sample = start_sample + segment_samples\n",
    "        \n",
    "        # Extract the current segment waveform\n",
    "        segment_waveform = full_waveform[start_sample:end_sample]\n",
    "\n",
    "        # Check if the segment is actually 5 seconds long (handles the last segment potentially being shorter if total length isn't an exact multiple)\n",
    "        if len(segment_waveform) < segment_samples and i != num_segments - 1:\n",
    "            # Should only happen if the loop condition is wrong, but a safeguard\n",
    "            continue \n",
    "            \n",
    "        # Extract the 1024-dim embedding\n",
    "        segment_embedding = extract_single_embedding(segment_waveform)\n",
    "        \n",
    "        # Predict the probability of the positive class for the segment\n",
    "        # predictions shape is (1, 1) -> get the single probability value\n",
    "        segment_prediction_prob = best_model.predict(segment_embedding, verbose=0)[0][0]\n",
    "\n",
    "        # Classify the segment based on the threshold\n",
    "        if segment_prediction_prob >= CLASSIFICATION_THRESHOLD:\n",
    "            segment_class = class_names[1] # Positive class\n",
    "        else:\n",
    "            segment_class = class_names[0] # Negative class\n",
    "            \n",
    "        segment_predictions.append(segment_class)\n",
    "        \n",
    "    # --- Step 4.3: Maximum Voting ---\n",
    "    if not segment_predictions:\n",
    "        return class_names[0], 0, 0, 0 # Default to negative if something went wrong\n",
    "\n",
    "    # Use Counter to find the most frequent class\n",
    "    vote_counts = Counter(segment_predictions)\n",
    "    final_class = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Calculate voting metrics\n",
    "    total_votes = len(segment_predictions)\n",
    "    positive_votes = vote_counts.get(class_names[1], 0)\n",
    "    negative_votes = vote_counts.get(class_names[0], 0)\n",
    "    \n",
    "    # The confidence is defined by the proportion of the winning votes\n",
    "    confidence_score = vote_counts[final_class] / total_votes\n",
    "    \n",
    "    return final_class, confidence_score, positive_votes, total_votes\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 5. Execution Block: Iterate Folder\n",
    "def predict_folder(folder_path, class_names):\n",
    "    \"\"\"\n",
    "    Iterates through all .wav files in a folder and applies the max voting prediction.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"\\n🚫 Error: Folder not found at {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"✨ Starting Prediction for folder: **{folder_path}** (5s Segments + Max Voting)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "            audio_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Run the segmentation and voting prediction\n",
    "            final_class, confidence, positive_votes, total_votes = segment_and_predict(audio_path, class_names)\n",
    "            \n",
    "            if final_class:\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'Total_Duration_s': f\"{librosa.get_duration(path=audio_path):.2f}\",\n",
    "                    'Total_Segments': total_votes,\n",
    "                    f'{class_names[1]}_Votes': positive_votes,\n",
    "                    'Final_Predicted_Class': final_class,\n",
    "                    'Max_Vote_Confidence': f\"{confidence:.4f}\",\n",
    "                })\n",
    "            \n",
    "    # Display results in a nice table\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"-\"*120)\n",
    "        print(\"✅ Prediction Summary (Segmented by 5s):\")\n",
    "        # Display the result columns\n",
    "        print(df.to_markdown(index=False))\n",
    "        print(\"\\nNote: The 'Max_Vote_Confidence' is the proportion of votes for the winning class.\")\n",
    "        print(\"-\"*120)\n",
    "        final_counts = Counter(df['Final_Predicted_Class'])\n",
    "        \n",
    "        print(\"\\n📊 **Final Prediction Tally Across All Files**\")\n",
    "        total_files = len(df)\n",
    "        print(f\"Total files processed: **{total_files}**\\n\")\n",
    "        \n",
    "        for class_name in class_names:\n",
    "            count = final_counts.get(class_name, 0)\n",
    "            percentage = (count / total_files) * 100 if total_files > 0 else 0\n",
    "            print(f\"   → **{class_name}**: {count} files ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n⚠️ No audio files processed.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "## 6. Execution Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a7a735b-a87e-469a-ba93-ad46826fb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✨ Starting Prediction for folder: **PARK_DATA/Mine** (5s Segments + Max Voting)\n",
      "================================================================================\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "✅ Prediction Summary (Segmented by 5s):\n",
      "| Filename      |   Total_Duration_s |   Total_Segments |   Parkinson_Positive_Votes | Final_Predicted_Class   |   Max_Vote_Confidence |\n",
      "|:--------------|-------------------:|-----------------:|---------------------------:|:------------------------|----------------------:|\n",
      "| Voice 022.wav |              59.3  |               11 |                          8 | Parkinson_Positive      |                0.7273 |\n",
      "| Voice 023.wav |              50.9  |               10 |                          7 | Parkinson_Positive      |                0.7    |\n",
      "| Voice 024.wav |              51.04 |               10 |                          4 | Parkinson_Negative      |                0.6    |\n",
      "\n",
      "Note: The 'Max_Vote_Confidence' is the proportion of votes for the winning class.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "📊 **Final Prediction Tally Across All Files**\n",
      "Total files processed: **3**\n",
      "\n",
      "   → **Parkinson_Negative**: 1 files (33.3%)\n",
      "   → **Parkinson_Positive**: 2 files (66.7%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# **DEFINE THE FOLDER YOU WANT TO TEST**\n",
    "TEST_FOLDER = 'PARK_DATA/Mine' \n",
    "\n",
    "# **DEFINE YOUR ACTUAL CLASS NAMES**\n",
    "# Class Names MUST be ordered as [Negative Class, Positive Class]\n",
    "CLASS_NAMES = ['Parkinson_Negative', 'Parkinson_Positive'] \n",
    "\n",
    "predict_folder(TEST_FOLDER, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e7d6a-6e8a-4ad6-9bf3-de1b6a067f56",
   "metadata": {},
   "source": [
    "### Threshold Analysis for the best 100 epoch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "636e5e16-8878-4ab1-87e4-e6fa1894f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: yamnet_pd_classifier_head.keras\n",
      "\n",
      "Generating predictions on validation set...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "--- Optimal Threshold Analysis ---\n",
      "Total Validation Samples: 129\n",
      "Positive Samples (1): 56\n",
      "Negative Samples (0): 73\n",
      "\n",
      "Best Classification Threshold: 0.4173\n",
      "Accuracy at this Threshold: 0.8295\n",
      "Balanced Accuracy (Optimal Metric): 0.8202\n",
      "\n",
      "Model AUC (original, threshold-independent): 0.8778\n",
      "\n",
      "Best threshold check complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_curve\n",
    "\n",
    "# --- Placeholder/Simulation for the Keras Model and Data ---\n",
    "# Since I cannot load your actual model or data, this section simulates the necessary\n",
    "# objects. If you run this in your environment, replace this with your actual loading code.\n",
    "\n",
    "# 1. Load the actual model head (Assuming it was saved as 'yamnet_pd_classifier_head.keras')\n",
    "try:\n",
    "    classifier = tf.keras.models.load_model(\"yamnet_pd_classifier_head.keras\")\n",
    "    print(\"Successfully loaded model: yamnet_pd_classifier_head.keras\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load the actual model: {e}\")\n",
    "    print(\"Using a placeholder model and dummy data for demonstration.\")\n",
    "    \n",
    "    # Define a simple placeholder model (only for simulation if loading fails)\n",
    "    class DummyClassifier(tf.keras.Model):\n",
    "        def predict(self, emb, **kwargs):\n",
    "            # Simulate probabilities for a binary classification task\n",
    "            num_samples = emb.shape[0]\n",
    "            # Generate random probabilities heavily skewed around 0.3 for demonstration\n",
    "            predictions = np.random.beta(a=1, b=3, size=(num_samples, 1))\n",
    "            return predictions\n",
    "\n",
    "    # Use the dummy model\n",
    "    classifier = DummyClassifier()\n",
    "    \n",
    "    # 2. Simulate validation data (Replace with your actual data)\n",
    "    num_samples = 500\n",
    "    embedding_dim = 1024 # YAMNet output\n",
    "    val_emb = np.random.rand(num_samples, embedding_dim).astype(np.float32)\n",
    "    \n",
    "    # Simulate true labels (y_val), with some imbalance (e.g., 80% negative, 20% positive)\n",
    "    y_val = np.random.choice([0, 1], size=num_samples, p=[0.8, 0.2]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Finds the optimal threshold for binary classification based on Youden's J statistic\n",
    "    (which maximizes the difference between True Positive Rate (Sensitivity) and False Positive Rate).\n",
    "    This often results in a better-balanced threshold than maximizing pure accuracy.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.array): True binary labels (0 or 1).\n",
    "        y_proba (np.array): Predicted probabilities for the positive class (shape N, 1).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_threshold, best_balanced_accuracy)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape probabilities if necessary (from N, 1 to N)\n",
    "    if y_proba.ndim > 1 and y_proba.shape[1] == 1:\n",
    "        y_proba = y_proba.flatten()\n",
    "\n",
    "    # Calculate ROC curve metrics (False Positive Rate, True Positive Rate, Thresholds)\n",
    "    # y_true must be 1D, y_proba must be 1D\n",
    "    fpr, tpr, thresholds = roc_curve(y_true.flatten(), y_proba)\n",
    "    \n",
    "    # Youden's J statistic (Maximize Sensitivity + Specificity - 1)\n",
    "    # Sensitivity = TPR\n",
    "    # Specificity = 1 - FPR\n",
    "    youden_j = tpr - fpr\n",
    "    \n",
    "    # Find the index of the best threshold\n",
    "    optimal_idx = np.argmax(youden_j)\n",
    "    \n",
    "    # Get the optimal threshold\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Calculate the Balanced Accuracy at this optimal threshold\n",
    "    y_pred_at_threshold = (y_proba >= optimal_threshold).astype(int)\n",
    "    best_balanced_acc = balanced_accuracy_score(y_true.flatten(), y_pred_at_threshold)\n",
    "    \n",
    "    # We also calculate the standard accuracy at this threshold for comparison\n",
    "    standard_acc = np.mean(y_true.flatten() == y_pred_at_threshold)\n",
    "    \n",
    "    return optimal_threshold, best_balanced_acc, standard_acc\n",
    "\n",
    "\n",
    "# ------------------------------- 7. Threshold Optimization ------------------------------------\n",
    "\n",
    "# 1. Get predicted probabilities from the model\n",
    "print(\"\\nGenerating predictions on validation set...\")\n",
    "# The .predict() output is typically a numpy array of probabilities\n",
    "y_pred_proba = classifier.predict(val_emb)\n",
    "\n",
    "\n",
    "# 2. Find the optimal threshold\n",
    "best_threshold, balanced_acc, standard_acc = find_optimal_threshold(y_val, y_pred_proba)\n",
    "\n",
    "# 3. Print the results\n",
    "print(\"\\n--- Optimal Threshold Analysis ---\")\n",
    "print(f\"Total Validation Samples: {len(y_val)}\")\n",
    "print(f\"Positive Samples (1): {np.sum(y_val)}\")\n",
    "print(f\"Negative Samples (0): {len(y_val) - np.sum(y_val)}\")\n",
    "\n",
    "print(f\"\\nBest Classification Threshold: {best_threshold:.4f}\")\n",
    "print(f\"Accuracy at this Threshold: {standard_acc:.4f}\")\n",
    "print(f\"Balanced Accuracy (Optimal Metric): {balanced_acc:.4f}\")\n",
    "\n",
    "# You can now use this best_threshold (e.g., 0.38) for all future binary classifications:\n",
    "# final_predictions = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Optional: Re-calculate and print AUC for completeness (as you did before)\n",
    "try:\n",
    "    val_loss, val_acc, val_auc = classifier.evaluate(val_emb, y_val, verbose=0)\n",
    "    print(f\"\\nModel AUC (original, threshold-independent): {val_auc:.4f}\")\n",
    "except:\n",
    "    # If the dummy model is used, Keras evaluate won't work easily\n",
    "    pass\n",
    "\n",
    "print(\"\\nBest threshold check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f90219-960a-4580-a6f6-79972acdef62",
   "metadata": {},
   "source": [
    "### COMBINING YAMNET AND CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "996ae7e5-5468-4ad4-a503-5c36ddf1b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "CLASSIFIER_HEAD_PATH = \"best_pd_yamnet_classifier.keras\"\n",
    "OUTPUT_TFLITE_PATH = \"best_pd_yamnet_classifier.tflite\"\n",
    "\n",
    "# YAMNet requires a specific input shape for the embedding features\n",
    "YAMNET_EMBEDDING_DIM = 1024\n",
    "\n",
    "def combine_and_convert_model():\n",
    "    \"\"\"\n",
    "    Combines the YAMNet feature extractor (represented by its input shape) \n",
    "    with the custom classifier head, converts the result to TFLite, and tests inference.\n",
    "    \"\"\"\n",
    "    print(\"--- Phase 1: Combining Models ---\")\n",
    "\n",
    "    # 1. Load the custom classifier head\n",
    "    try:\n",
    "        classifier_head = tf.keras.models.load_model(CLASSIFIER_HEAD_PATH)\n",
    "        print(f\"Loaded custom classifier head from: {CLASSIFIER_HEAD_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading classifier head: {e}\")\n",
    "        print(\"Creating a dummy classifier head for demonstration.\")\n",
    "        # Create a placeholder classifier head for testing the script structure\n",
    "        classifier_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(YAMNET_EMBEDDING_DIM,)),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid', name='final_output')\n",
    "        ])\n",
    "\n",
    "    # 2. Define the Input Layer for the full model\n",
    "    # The input to the full model *after* feature extraction is the YAMNet embedding (1024 features)\n",
    "    # The actual YAMNet model (audio -> embedding) is often deployed separately or included \n",
    "    # as a pre-processing step in the final mobile pipeline. For Keras combining, we start \n",
    "    # from the embedding layer.\n",
    "    embedding_input = tf.keras.Input(shape=(YAMNET_EMBEDDING_DIM,), name='yamnet_embedding_input')\n",
    "\n",
    "    # 3. Connect the input to the classifier head\n",
    "    final_output = classifier_head(embedding_input)\n",
    "\n",
    "    # 4. Create the full Keras model\n",
    "    full_model = tf.keras.Model(inputs=embedding_input, outputs=final_output, name='full_pd_yamnet_classifier')\n",
    "    full_model.summary()\n",
    "\n",
    "    print(\"\\n--- Phase 2: Converting to TFLite ---\")\n",
    "\n",
    "    # Use the TFLite converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(full_model)\n",
    "\n",
    "    # Optimization is crucial for mobile devices\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Save the TFLite model file\n",
    "        with open(OUTPUT_TFLITE_PATH, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"✅ Successfully converted and saved TFLite model to: {OUTPUT_TFLITE_PATH}\")\n",
    "\n",
    "        # 5. Test the TFLite model inference\n",
    "        test_tflite_model(tflite_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TFLite Conversion Failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e57e8bc-fb72-49db-ae05-70fd7c713806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Combining Models ---\n",
      "Loaded custom classifier head from: best_pd_yamnet_classifier.keras\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"full_pd_yamnet_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"full_pd_yamnet_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ yamnet_embedding_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">689,153</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ yamnet_embedding_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ functional (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │         \u001b[38;5;34m689,153\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689,153</span> (2.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m689,153\u001b[0m (2.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">689,153</span> (2.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m689,153\u001b[0m (2.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Converting to TFLite ---\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Felix\\AppData\\Local\\Temp\\tmp4iztsgr_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Felix\\AppData\\Local\\Temp\\tmp4iztsgr_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Felix\\AppData\\Local\\Temp\\tmp4iztsgr_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1024), dtype=tf.float32, name='yamnet_embedding_input')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1525360982544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525360982736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525360982928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1525863909520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1527316836432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1527316831824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1527316832592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1527316830096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ Successfully converted and saved TFLite model to: best_pd_yamnet_classifier.tflite\n",
      "\n",
      "--- Phase 3: Testing TFLite Inference ---\n",
      "TFLite Input Shape: [   1 1024]\n",
      "TFLite Output Shape: [1 1]\n",
      "TFLite Output Value (Probability): 0.9698\n",
      "✅ TFLite model test successful! Ready for mobile integration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "def test_tflite_model(tflite_model):\n",
    "    \"\"\"\n",
    "    Tests the TFLite model by performing inference on a dummy input array.\n",
    "    This verifies that the model is runnable and has the correct input/output shape.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Phase 3: Testing TFLite Inference ---\")\n",
    "    \n",
    "    # Initialize the TFLite Interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Create a dummy input embedding (simulating one 0.96s audio segment)\n",
    "    input_shape = input_details[0]['shape'] # Should be (1, 1024)\n",
    "    # Ensure the input is float32, which is standard for Keras/TFLite\n",
    "    dummy_input = np.random.rand(*input_shape).astype(np.float32)\n",
    "\n",
    "    # Set the input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], dummy_input)\n",
    "\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get the output tensor (the prediction probability)\n",
    "    tflite_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    print(f\"TFLite Input Shape: {input_details[0]['shape']}\")\n",
    "    print(f\"TFLite Output Shape: {output_details[0]['shape']}\")\n",
    "    print(f\"TFLite Output Value (Probability): {tflite_output[0][0]:.4f}\")\n",
    "\n",
    "    if tflite_output.shape == (1, 1) and tflite_output.dtype == np.float32:\n",
    "        print(\"✅ TFLite model test successful! Ready for mobile integration.\")\n",
    "    else:\n",
    "        print(\"❌ TFLite model test failed: Output shape or type is incorrect.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    combine_and_convert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22b33847-224c-4198-8577-7d3b0db9e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# =================================================================================\n",
    "# ⚠️ 1. Configuration Constants (MUST MATCH YOUR TRAINING SETUP)\n",
    "# =================================================================================\n",
    "\n",
    "# Path to the TFLite model generated in the previous step\n",
    "TFLITE_MODEL_PATH = \"best_pd_yamnet_classifier.tflite\"\n",
    "\n",
    "# The duration of the segment the model was trained on\n",
    "SEGMENT_DURATION_SECONDS = 5\n",
    "\n",
    "# YAMNet/Audio Requirements\n",
    "YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1' \n",
    "TARGET_SAMPLE_RATE = 16000 \n",
    "YAMNET_EMBEDDING_DIM = 1024\n",
    "\n",
    "# IMPORTANT: Replace this with the optimal threshold you found previously (e.g., 0.3800)\n",
    "OPTIMAL_THRESHOLD = 0.49 \n",
    "\n",
    "# Define class names for output reporting\n",
    "CLASS_NAMES = [\"Healthy (H)\", \"Parkinson's Disease (PD)\"]\n",
    "\n",
    "# =================================================================================\n",
    "# ⚠️ 2. Jupyter Setup: Define your audio folder path here\n",
    "# =================================================================================\n",
    "\n",
    "# Set the path to the folder containing your test audio files (WAV, MP3, etc.)\n",
    "# EXAMPLE: AUDIO_FOLDER_PATH = './my_test_audio_data/'\n",
    "AUDIO_FOLDER_PATH = 'PARK_DATA/PD_segments' \n",
    "\n",
    "# =================================================================================\n",
    "# 3. Core Functions\n",
    "# =================================================================================\n",
    "\n",
    "def load_yamnet_feature_extractor():\n",
    "    \"\"\"Loads the YAMNet model from TensorFlow Hub to generate embeddings.\"\"\"\n",
    "    try:\n",
    "        yamnet_model = hub.load(YAMNET_MODEL_HANDLE)\n",
    "        print(\"YAMNet feature extractor loaded successfully from TensorFlow Hub.\")\n",
    "        return yamnet_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YAMNet feature extractor: {e}\")\n",
    "        return None\n",
    "\n",
    "def run_tflite_inference(tflite_model_content: bytes, embedding: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Performs inference on a single (1, 1024) embedding using the TFLite interpreter.\n",
    "    Returns the prediction probability (float).\n",
    "    \"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model_content)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    \n",
    "    # Input must be shaped as (1, 1024) and be of type float32\n",
    "    embedding = embedding.astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details['index'], embedding)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get the single prediction probability\n",
    "    probability = interpreter.get_tensor(output_details['index'])[0][0]\n",
    "    return probability\n",
    "\n",
    "def extract_single_embedding(yamnet_model: tf.Module, waveform_segment: tf.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts the YAMNet embedding from a waveform segment and averages it.\n",
    "    Input: (N,) Tensor of audio samples (e.g., 5 seconds of 16kHz audio)\n",
    "    Output: (1, 1024) numpy array\n",
    "    \"\"\"\n",
    "    # YAMNet handles the internal 0.96s frames and returns multiple embeddings\n",
    "    # embeddings shape: (Number_of_sub_frames, 1024)\n",
    "    _, embeddings, _ = yamnet_model(waveform_segment)\n",
    "    \n",
    "    # Average the embeddings across all sub-frames to get a single (1, 1024) vector.\n",
    "    avg_embedding = np.mean(embeddings.numpy(), axis=0, keepdims=True)\n",
    "    return avg_embedding\n",
    "\n",
    "def segment_and_predict(audio_path: str, yamnet_model: tf.Module, tflite_model_content: bytes) -> Tuple[str, float, int, int]:\n",
    "    \"\"\"\n",
    "    Loads a long audio file, splits it into 5-second segments,\n",
    "    predicts the class for each segment using the averaged embedding, \n",
    "    and uses Max Voting for the final result.\n",
    "    Returns: (final_class, confidence_score, positive_votes, total_votes)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing: {os.path.basename(audio_path)} ---\")\n",
    "    \n",
    "    # --- Step 1: Load the full audio file ---\n",
    "    try:\n",
    "        # Load audio, resample to 16kHz, mono\n",
    "        full_waveform, sr = librosa.load(audio_path, sr=TARGET_SAMPLE_RATE, mono=True)\n",
    "        full_waveform_tf = tf.constant(full_waveform, dtype=tf.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading {audio_path}: {e}\")\n",
    "        return CLASS_NAMES[0], 0.0, 0, 0\n",
    "    \n",
    "    total_samples = len(full_waveform)\n",
    "    segment_samples = SEGMENT_DURATION_SECONDS * sr\n",
    "    \n",
    "    # Calculate number of non-overlapping segments\n",
    "    num_segments = total_samples // segment_samples\n",
    "    \n",
    "    if num_segments == 0:\n",
    "        # If the file is shorter than 5 seconds, treat the whole file as one segment\n",
    "        num_segments = 1\n",
    "        # No need to truncate, full_waveform is used below\n",
    "    elif total_samples % segment_samples != 0:\n",
    "        # Ensure we only process full 5-second segments by truncating the end\n",
    "        full_waveform_tf = full_waveform_tf[:num_segments * segment_samples]\n",
    "\n",
    "    segment_predictions_label: List[str] = []\n",
    "    segment_prediction_probs: List[float] = []\n",
    "\n",
    "    # --- Step 2: Iterate through segments ---\n",
    "    for i in range(num_segments):\n",
    "        \n",
    "        # Get the segment waveform (5s chunk)\n",
    "        if num_segments == 1 and total_samples < segment_samples:\n",
    "            # Case 1: File < 5s. segment_waveform is the full waveform\n",
    "            segment_waveform = full_waveform_tf\n",
    "        else:\n",
    "            # Case 2: File >= 5s. Slice the 5s chunk.\n",
    "            start_sample = i * segment_samples\n",
    "            end_sample = start_sample + segment_samples\n",
    "            segment_waveform = full_waveform_tf[start_sample:end_sample]\n",
    "\n",
    "        # Extract the single averaged 1024-dim embedding for this 5s chunk\n",
    "        segment_embedding = extract_single_embedding(yamnet_model, segment_waveform)\n",
    "        \n",
    "        # Predict the probability of the positive class for the segment using the TFLite model\n",
    "        segment_prediction_prob = run_tflite_inference(tflite_model_content, segment_embedding)\n",
    "        segment_prediction_probs.append(segment_prediction_prob)\n",
    "\n",
    "        # Classify the segment based on the optimal threshold\n",
    "        if segment_prediction_prob >= OPTIMAL_THRESHOLD:\n",
    "            segment_class = CLASS_NAMES[1] # Positive class (PD)\n",
    "        else:\n",
    "            segment_class = CLASS_NAMES[0] # Negative class (Healthy)\n",
    "            \n",
    "        segment_predictions_label.append(segment_class)\n",
    "    \n",
    "    print(f\"Total audio duration: {total_samples/sr:.2f}s, processed as {num_segments} segment(s).\")\n",
    "    print(f\"Segment prediction probabilities: {np.array(segment_prediction_probs).round(4)}\")\n",
    "\n",
    "\n",
    "    # --- Step 3: Maximum Voting ---\n",
    "    if not segment_predictions_label:\n",
    "        return CLASS_NAMES[0], 0.0, 0, 0\n",
    "\n",
    "    # Use Counter to find the most frequent class\n",
    "    vote_counts = Counter(segment_predictions_label)\n",
    "    final_class = vote_counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Calculate voting metrics\n",
    "    total_votes = len(segment_predictions_label)\n",
    "    positive_votes = vote_counts.get(CLASS_NAMES[1], 0)\n",
    "    \n",
    "    # The confidence is defined by the proportion of the winning votes\n",
    "    confidence_score = vote_counts[final_class] / total_votes\n",
    "    \n",
    "    return final_class, confidence_score, positive_votes, total_votes\n",
    "\n",
    "def predict_folder(folder_path: str, yamnet_model: tf.Module, tflite_model_content: bytes):\n",
    "    \"\"\"\n",
    "    Iterates through all supported audio files in a folder and applies the max voting prediction.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"\\n🚫 Error: Folder not found at {folder_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"✨ Starting Prediction for folder: **{folder_path}** (5s Averaged Embeddings + Max Voting)\")\n",
    "    print(f\"Using Optimal Threshold: {OPTIMAL_THRESHOLD:.4f}\")\n",
    "    print(\"=\"*90)\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.lower().endswith(('.wav', '.mp3', '.flac', '.m4a')):\n",
    "            audio_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Run the segmentation and voting prediction\n",
    "            final_class, confidence, positive_votes, total_votes = segment_and_predict(\n",
    "                audio_path, yamnet_model, tflite_model_content\n",
    "            )\n",
    "            \n",
    "            if final_class:\n",
    "                try:\n",
    "                    duration = librosa.get_duration(path=audio_path)\n",
    "                except Exception:\n",
    "                    duration = 0.0\n",
    "\n",
    "                results.append({\n",
    "                    'Filename': filename,\n",
    "                    'Duration (s)': f\"{duration:.2f}\",\n",
    "                    'Total Segments': total_votes,\n",
    "                    f'{CLASS_NAMES[1].split(\" \")[0]}_Votes': positive_votes,\n",
    "                    'Final Predicted Class': final_class,\n",
    "                    'Max Vote Confidence': f\"{confidence:.4f}\",\n",
    "                })\n",
    "\n",
    "    # Display results in a nice table\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"-\"*120)\n",
    "        print(\"✅ Prediction Summary (5s Averaging + Max Voting):\")\n",
    "        \n",
    "        # Display the result columns\n",
    "        print(df.to_markdown(index=False))\n",
    "        print(\"\\nNote: 'Max Vote Confidence' is the proportion of votes for the winning class.\")\n",
    "        print(\"-\"*120)\n",
    "        final_counts = Counter(df['Final Predicted Class'])\n",
    "        \n",
    "        print(\"\\n📊 **Overall Prediction Tally Across All Files**\")\n",
    "        total_files = len(df)\n",
    "        print(f\"Total files processed: **{total_files}**\\n\")\n",
    "        \n",
    "        for class_name in CLASS_NAMES:\n",
    "            count = final_counts.get(class_name, 0)\n",
    "            percentage = (count / total_files) * 100 if total_files > 0 else 0\n",
    "            print(f\"    → **{class_name}**: {count} files ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n⚠️ No supported audio files processed.\")\n",
    "\n",
    "def run_prediction_test(folder_path: str):\n",
    "    \"\"\"Main execution function to run the prediction pipeline.\"\"\"\n",
    "    try:\n",
    "        # Load TFLite model content once\n",
    "        with open(TFLITE_MODEL_PATH, \"rb\") as f:\n",
    "            tflite_model_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: TFLite model not found at {TFLITE_MODEL_PATH}. Ensure the file exists.\")\n",
    "        return\n",
    "        \n",
    "    # Load the YAMNet feature extractor once\n",
    "    yamnet_model = load_yamnet_feature_extractor()\n",
    "    if yamnet_model is None:\n",
    "        return\n",
    "\n",
    "    # --- Run Prediction ---\n",
    "    predict_folder(folder_path, yamnet_model, tflite_model_content)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# ⚠️ 4. EXECUTION CELL\n",
    "# \n",
    "# To run this in Jupyter, ensure the TFLITE_MODEL_PATH and AUDIO_FOLDER_PATH\n",
    "# variables are set correctly above, and then execute this line:\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# run_prediction_test(AUDIO_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7591e9df-bb64-4dae-9d91-44c544af830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAMNet feature extractor loaded successfully from TensorFlow Hub.\n",
      "\n",
      "==========================================================================================\n",
      "✨ Starting Prediction for folder: **PARK_DATA/PD_segments** (5s Averaged Embeddings + Max Voting)\n",
      "Using Optimal Threshold: 0.4900\n",
      "==========================================================================================\n",
      "\n",
      "--- Analyzing: ID02_pd_2_0_0.wav_seg0.wav ---\n",
      "Total audio duration: 78.78s, processed as 15 segment(s).\n",
      "Segment prediction probabilities: [0.4443 0.4585 0.5436 0.4717 0.4571 0.7162 0.5074 0.5442 0.5049 0.442\n",
      " 0.4236 0.5278 0.5318 0.45   0.5222]\n",
      "\n",
      "--- Analyzing: ID02_pd_2_0_0.wav_seg1.wav ---\n",
      "Total audio duration: 43.16s, processed as 8 segment(s).\n",
      "Segment prediction probabilities: [0.5757 0.4943 0.5521 0.5108 0.5461 0.8296 0.4915 0.4888]\n",
      "\n",
      "--- Analyzing: ID04_pd_2_0_1.wav_seg0.wav ---\n",
      "Total audio duration: 52.01s, processed as 10 segment(s).\n",
      "Segment prediction probabilities: [0.4686 0.5379 0.5094 0.5296 0.5117 0.5289 0.6236 0.4664 0.5095 0.47  ]\n",
      "\n",
      "--- Analyzing: ID04_pd_2_0_1.wav_seg1.wav ---\n",
      "Total audio duration: 27.85s, processed as 5 segment(s).\n",
      "Segment prediction probabilities: [0.5509 0.4874 0.5641 0.5385 0.4658]\n",
      "\n",
      "--- Analyzing: ID06_pd_3_1_1.wav_seg0.wav ---\n",
      "Total audio duration: 92.62s, processed as 18 segment(s).\n",
      "Segment prediction probabilities: [0.5069 0.5094 0.4838 0.5027 0.5305 0.6454 0.583  0.57   0.5253 0.5451\n",
      " 0.5778 0.5339 0.5246 0.5367 0.6205 0.5412 0.5178 0.5183]\n",
      "\n",
      "--- Analyzing: ID06_pd_3_1_1.wav_seg1.wav ---\n",
      "Total audio duration: 43.61s, processed as 8 segment(s).\n",
      "Segment prediction probabilities: [0.5715 0.5807 0.5095 0.5304 0.558  0.5936 0.7292 0.5611]\n",
      "\n",
      "--- Analyzing: ID07_pd_2_0_0.wav_seg0.wav ---\n",
      "Total audio duration: 45.03s, processed as 9 segment(s).\n",
      "Segment prediction probabilities: [0.5326 0.5662 0.5511 0.7661 0.5744 0.4585 0.8904 0.556  0.5352]\n",
      "\n",
      "--- Analyzing: ID07_pd_2_0_0.wav_seg1.wav ---\n",
      "Total audio duration: 70.97s, processed as 14 segment(s).\n",
      "Segment prediction probabilities: [0.5112 0.5176 0.8257 0.8097 0.5012 0.9784 0.5291 0.5293 0.5329 0.4249\n",
      " 0.5344 0.5362 0.4732 0.5112]\n",
      "\n",
      "--- Analyzing: ID13_pd_3_2_2.wav_seg0.wav ---\n",
      "Total audio duration: 59.11s, processed as 11 segment(s).\n",
      "Segment prediction probabilities: [0.6184 0.5423 0.5411 0.5671 0.5608 0.5097 0.4922 0.5334 0.4904 0.5248\n",
      " 0.5165]\n",
      "\n",
      "--- Analyzing: ID16_pd_2_0_0.wav_seg0.wav ---\n",
      "Total audio duration: 48.26s, processed as 9 segment(s).\n",
      "Segment prediction probabilities: [0.6353 0.665  0.7943 0.4057 0.3649 0.3107 0.3546 0.7041 0.3795]\n",
      "\n",
      "--- Analyzing: ID16_pd_2_0_0.wav_seg1.wav ---\n",
      "Total audio duration: 81.54s, processed as 16 segment(s).\n",
      "Segment prediction probabilities: [0.3977 0.3512 0.328  0.4198 0.7381 0.258  0.3792 0.2681 0.3138 0.296\n",
      " 0.3277 0.3031 0.296  0.6706 0.3487 0.4027]\n",
      "\n",
      "--- Analyzing: ID18_pd_4_3_3.wav_seg0.wav ---\n",
      "Total audio duration: 75.03s, processed as 15 segment(s).\n",
      "Segment prediction probabilities: [0.739  0.9859 0.9263 0.8563 0.9988 0.9996 0.5314 0.9858 0.9205 0.9978\n",
      " 0.9837 0.8985 0.9842 0.8017 0.7929]\n",
      "\n",
      "--- Analyzing: ID20_pd_3_0_1.wav_seg0.wav ---\n",
      "Total audio duration: 40.95s, processed as 8 segment(s).\n",
      "Segment prediction probabilities: [0.7517 0.8515 0.5601 0.7349 0.5694 0.562  0.8205 0.5873]\n",
      "\n",
      "--- Analyzing: ID20_pd_3_0_1.wav_seg1.wav ---\n",
      "Total audio duration: 67.32s, processed as 13 segment(s).\n",
      "Segment prediction probabilities: [0.587  0.6054 0.7535 0.7559 0.6439 0.6329 0.6013 0.6271 0.6651 0.8431\n",
      " 0.6706 0.6227 0.6271]\n",
      "\n",
      "--- Analyzing: ID24_pd_2_0_0.wav_seg0.wav ---\n",
      "Total audio duration: 36.07s, processed as 7 segment(s).\n",
      "Segment prediction probabilities: [0.4876 0.5173 0.5563 0.5057 0.5683 0.5396 0.4982]\n",
      "\n",
      "--- Analyzing: ID24_pd_2_0_0.wav_seg1.wav ---\n",
      "Total audio duration: 69.44s, processed as 13 segment(s).\n",
      "Segment prediction probabilities: [0.5627 0.5923 0.518  0.4577 0.5097 0.4145 0.5652 0.4839 0.5345 0.4578\n",
      " 0.433  0.4999 0.5438]\n",
      "\n",
      "--- Analyzing: ID27_pd_4_1_1.wav_seg0.wav ---\n",
      "Total audio duration: 57.87s, processed as 11 segment(s).\n",
      "Segment prediction probabilities: [0.691  0.5615 0.4815 0.4957 0.5443 0.5303 0.5268 0.4134 0.5218 0.5173\n",
      " 0.4935]\n",
      "\n",
      "--- Analyzing: ID29_pd_3_1_2.wav_seg0.wav ---\n",
      "Total audio duration: 49.48s, processed as 9 segment(s).\n",
      "Segment prediction probabilities: [0.8532 0.9994 0.4784 0.8798 0.8579 0.9252 0.9979 0.8278 0.4909]\n",
      "\n",
      "--- Analyzing: ID29_pd_3_1_2.wav_seg1.wav ---\n",
      "Total audio duration: 35.17s, processed as 7 segment(s).\n",
      "Segment prediction probabilities: [0.7675 0.9467 0.8146 0.9091 0.8082 0.5602 0.8598]\n",
      "\n",
      "--- Analyzing: ID29_pd_3_1_2.wav_seg2.wav ---\n",
      "Total audio duration: 37.85s, processed as 7 segment(s).\n",
      "Segment prediction probabilities: [0.5606 0.4921 0.4976 0.7925 0.7233 0.9747 0.5568]\n",
      "\n",
      "--- Analyzing: ID30_pd_2_1_1.wav_seg0.wav ---\n",
      "Total audio duration: 42.72s, processed as 8 segment(s).\n",
      "Segment prediction probabilities: [0.4015 0.3412 0.3999 0.2812 0.4648 0.3444 0.4412 0.4224]\n",
      "\n",
      "--- Analyzing: ID30_pd_2_1_1.wav_seg1.wav ---\n",
      "Total audio duration: 65.39s, processed as 13 segment(s).\n",
      "Segment prediction probabilities: [0.3898 0.3228 0.3525 0.2997 0.2845 0.2918 0.3252 0.3188 0.2942 0.2921\n",
      " 0.3955 0.3284 0.2985]\n",
      "\n",
      "--- Analyzing: ID32_pd_3_1_1.wav_seg0.wav ---\n",
      "Total audio duration: 53.98s, processed as 10 segment(s).\n",
      "Segment prediction probabilities: [0.5391 0.596  0.531  0.5685 0.5322 0.5181 0.5262 0.5411 0.5386 0.5162]\n",
      "\n",
      "--- Analyzing: ID33_pd_3_2_2.wav_seg0.wav ---\n",
      "Total audio duration: 93.39s, processed as 18 segment(s).\n",
      "Segment prediction probabilities: [0.4688 0.4905 0.5875 0.8257 0.9227 0.4867 0.532  0.9412 0.4179 0.901\n",
      " 0.9138 0.8768 0.9485 0.9121 0.9799 0.4377 0.7068 0.829 ]\n",
      "\n",
      "--- Analyzing: ID34_pd_2_0_0.wav_seg0.wav ---\n",
      "Total audio duration: 35.70s, processed as 7 segment(s).\n",
      "Segment prediction probabilities: [0.324  0.2713 0.3509 0.3134 0.3325 0.3796 0.3238]\n",
      "\n",
      "--- Analyzing: ID34_pd_2_0_0.wav_seg1.wav ---\n",
      "Total audio duration: 63.02s, processed as 12 segment(s).\n",
      "Segment prediction probabilities: [0.388  0.2786 0.2597 0.3575 0.303  0.254  0.3689 0.4245 0.2802 0.4139\n",
      " 0.3293 0.33  ]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "✅ Prediction Summary (5s Averaging + Max Voting):\n",
      "| Filename                   |   Duration (s) |   Total Segments |   Parkinson's_Votes | Final Predicted Class    |   Max Vote Confidence |\n",
      "|:---------------------------|---------------:|-----------------:|--------------------:|:-------------------------|----------------------:|\n",
      "| ID02_pd_2_0_0.wav_seg0.wav |          78.78 |               15 |                   8 | Parkinson's Disease (PD) |                0.5333 |\n",
      "| ID02_pd_2_0_0.wav_seg1.wav |          43.16 |                8 |                   7 | Parkinson's Disease (PD) |                0.875  |\n",
      "| ID04_pd_2_0_1.wav_seg0.wav |          52.01 |               10 |                   7 | Parkinson's Disease (PD) |                0.7    |\n",
      "| ID04_pd_2_0_1.wav_seg1.wav |          27.85 |                5 |                   3 | Parkinson's Disease (PD) |                0.6    |\n",
      "| ID06_pd_3_1_1.wav_seg0.wav |          92.62 |               18 |                  17 | Parkinson's Disease (PD) |                0.9444 |\n",
      "| ID06_pd_3_1_1.wav_seg1.wav |          43.61 |                8 |                   8 | Parkinson's Disease (PD) |                1      |\n",
      "| ID07_pd_2_0_0.wav_seg0.wav |          45.03 |                9 |                   8 | Parkinson's Disease (PD) |                0.8889 |\n",
      "| ID07_pd_2_0_0.wav_seg1.wav |          70.97 |               14 |                  12 | Parkinson's Disease (PD) |                0.8571 |\n",
      "| ID13_pd_3_2_2.wav_seg0.wav |          59.11 |               11 |                  11 | Parkinson's Disease (PD) |                1      |\n",
      "| ID16_pd_2_0_0.wav_seg0.wav |          48.26 |                9 |                   4 | Healthy (H)              |                0.5556 |\n",
      "| ID16_pd_2_0_0.wav_seg1.wav |          81.54 |               16 |                   2 | Healthy (H)              |                0.875  |\n",
      "| ID18_pd_4_3_3.wav_seg0.wav |          75.03 |               15 |                  15 | Parkinson's Disease (PD) |                1      |\n",
      "| ID20_pd_3_0_1.wav_seg0.wav |          40.95 |                8 |                   8 | Parkinson's Disease (PD) |                1      |\n",
      "| ID20_pd_3_0_1.wav_seg1.wav |          67.32 |               13 |                  13 | Parkinson's Disease (PD) |                1      |\n",
      "| ID24_pd_2_0_0.wav_seg0.wav |          36.07 |                7 |                   6 | Parkinson's Disease (PD) |                0.8571 |\n",
      "| ID24_pd_2_0_0.wav_seg1.wav |          69.44 |               13 |                   8 | Parkinson's Disease (PD) |                0.6154 |\n",
      "| ID27_pd_4_1_1.wav_seg0.wav |          57.87 |               11 |                   9 | Parkinson's Disease (PD) |                0.8182 |\n",
      "| ID29_pd_3_1_2.wav_seg0.wav |          49.48 |                9 |                   8 | Parkinson's Disease (PD) |                0.8889 |\n",
      "| ID29_pd_3_1_2.wav_seg1.wav |          35.17 |                7 |                   7 | Parkinson's Disease (PD) |                1      |\n",
      "| ID29_pd_3_1_2.wav_seg2.wav |          37.85 |                7 |                   7 | Parkinson's Disease (PD) |                1      |\n",
      "| ID30_pd_2_1_1.wav_seg0.wav |          42.72 |                8 |                   0 | Healthy (H)              |                1      |\n",
      "| ID30_pd_2_1_1.wav_seg1.wav |          65.39 |               13 |                   0 | Healthy (H)              |                1      |\n",
      "| ID32_pd_3_1_1.wav_seg0.wav |          53.98 |               10 |                  10 | Parkinson's Disease (PD) |                1      |\n",
      "| ID33_pd_3_2_2.wav_seg0.wav |          93.39 |               18 |                  14 | Parkinson's Disease (PD) |                0.7778 |\n",
      "| ID34_pd_2_0_0.wav_seg0.wav |          35.7  |                7 |                   0 | Healthy (H)              |                1      |\n",
      "| ID34_pd_2_0_0.wav_seg1.wav |          63.02 |               12 |                   0 | Healthy (H)              |                1      |\n",
      "\n",
      "Note: 'Max Vote Confidence' is the proportion of votes for the winning class.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "📊 **Overall Prediction Tally Across All Files**\n",
      "Total files processed: **26**\n",
      "\n",
      "    → **Healthy (H)**: 6 files (23.1%)\n",
      "    → **Parkinson's Disease (PD)**: 20 files (76.9%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "run_prediction_test(AUDIO_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb66b300-a5ae-410f-a387-88899ba1d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created yamnet_feature_extractor.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the full YAMNet model\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1') \n",
    "# The loaded object is a callable that returns (scores, embeddings, spectrogram)\n",
    "\n",
    "# 2. Identify the embedding output function\n",
    "def yamnet_embedding_extractor(waveform):\n",
    "    # Call the model with the 16kHz waveform input\n",
    "    scores, embeddings, spectrogram = yamnet_model(waveform)\n",
    "    \n",
    "    # We only care about the embedding tensor (index 1)\n",
    "    return embeddings\n",
    "\n",
    "# 3. Define the input signature for TFLite conversion\n",
    "# Input shape must be None for dynamic length audio, or a specific shape \n",
    "# if you want a fixed-size buffer. Here we use None for flexibility.\n",
    "# YAMNet input is 1D float32 waveform (16000 samples per second)\n",
    "concrete_func = tf.function(yamnet_embedding_extractor,\n",
    "                            input_signature=[\n",
    "                                tf.TensorSpec(shape=[None], dtype=tf.float32)\n",
    "                            ])\n",
    "concrete_func = concrete_func.get_concrete_function()\n",
    "\n",
    "\n",
    "# 4. Convert the model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], \n",
    "                                                          yamnet_model.signatures['serving_default'])\n",
    "# Note: You may need to set specific optimizations for TFLite conversion here.\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 5. Save the TFLite file\n",
    "with open('yamnet_feature_extractor.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Successfully created yamnet_feature_extractor.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d288a-011a-4769-a696-0b00f2baa645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bb0394-e32d-4f54-9b05-0bcbdd706988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yamnet_5s_embed_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yamnet_5s_embed_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model: yamnet_5s_mean_embedding.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "class Yamnet5sEmbedding(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Track YAMNet as an attribute\n",
    "        self.yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\n",
    "    def embed(self, waveform):\n",
    "        \"\"\"\n",
    "        waveform: 1D tensor, exactly 5 seconds (80,000 samples)\n",
    "        returns: 1x1024 tensor embedding\n",
    "        \"\"\"\n",
    "        # Use the tracked YAMNet\n",
    "        scores, embeddings, _ = self.yamnet(waveform)\n",
    "        pooled = tf.reduce_mean(embeddings, axis=0)  # (1024,)\n",
    "        pooled = tf.expand_dims(pooled, axis=0)      # (1, 1024)\n",
    "        return {\"embedding\": pooled}\n",
    "\n",
    "# Create module\n",
    "module = Yamnet5sEmbedding()\n",
    "\n",
    "# Save as SavedModel\n",
    "tf.saved_model.save(\n",
    "    module,\n",
    "    \"yamnet_5s_embed_model\",\n",
    "    signatures={\"serving_default\": module.embed}\n",
    ")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"yamnet_5s_embed_model\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite\n",
    "with open(\"yamnet_5s_mean_embedding.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved TFLite model: yamnet_5s_mean_embedding.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c6c934-f5ee-42a6-bcfc-6b6bf1cb04c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 1024)\n",
      "First 5 elements: [0.         0.         0.00182058 0.08442353 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"yamnet_5s_mean_embedding.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Dummy 5-second audio (batch size 1)\n",
    "dummy_audio = np.random.randn(1, 80000).astype(np.float32)  # shape (1, 80000)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], dummy_audio)\n",
    "interpreter.invoke()\n",
    "\n",
    "embedding = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Output shape:\", embedding.shape)  # (1, 1024)\n",
    "print(\"First 5 elements:\", embedding[0, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "230e424a-51cb-477d-a4d4-5055b1618724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yamnet_5s_embed_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yamnet_5s_embed_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TFLite model with 2D input (1, 80000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "class Yamnet5sEmbedding(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[1, 80000], dtype=tf.float32)])\n",
    "    def embed(self, waveform):\n",
    "        \"\"\"\n",
    "        waveform: [1, 80000] batch of 1 audio clip\n",
    "        returns: 1x1024 tensor embedding\n",
    "        \"\"\"\n",
    "        # Remove batch dim for YAMNet\n",
    "        waveform_1d = tf.squeeze(waveform, axis=0)  # shape (80000,)\n",
    "        scores, embeddings, _ = self.yamnet(waveform_1d)\n",
    "        pooled = tf.reduce_mean(embeddings, axis=0)\n",
    "        pooled = tf.expand_dims(pooled, axis=0)    # (1, 1024)\n",
    "        return {\"embedding\": pooled}\n",
    "\n",
    "# Save SavedModel\n",
    "module = Yamnet5sEmbedding()\n",
    "tf.saved_model.save(module, \"yamnet_5s_embed_model\", signatures={\"serving_default\": module.embed})\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"yamnet_5s_embed_model\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"yamnet_5s_mean_embedding.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved TFLite model with 2D input (1, 80000)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "262db919-3e15-4f63-beb9-c30bf92a548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 1024)\n",
      "First 5 elements: [0.0792807  0.24947849 0.1258863  0.06407533 0.06327682]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import soundfile as sf  # pip install soundfile\n",
    "\n",
    "# --- 1. Load 5-second audio file ---\n",
    "audio_file = \"dataset_5sec/HC/ID00_hc_0_0_0.wav_seg0_0000.wav\"\n",
    "\n",
    "# Load waveform, convert to float32, ensure 16 kHz\n",
    "waveform, sr = sf.read(audio_file, dtype='float32')\n",
    "\n",
    "# Resample if needed\n",
    "if sr != 16000:\n",
    "    import librosa\n",
    "    waveform = librosa.resample(waveform, orig_sr=sr, target_sr=16000)\n",
    "    sr = 16000\n",
    "\n",
    "# Ensure mono\n",
    "if waveform.ndim > 1:\n",
    "    waveform = np.mean(waveform, axis=1)\n",
    "\n",
    "# Check length: must be 5 seconds → 80000 samples\n",
    "if len(waveform) != 16000*5:\n",
    "    raise ValueError(f\"Audio length is {len(waveform)/sr:.2f}s, must be 5s\")\n",
    "\n",
    "# Add batch dimension for TFLite\n",
    "waveform = np.expand_dims(waveform, axis=0)  # shape (1, 80000)\n",
    "\n",
    "# --- 2. Load TFLite model ---\n",
    "interpreter = tf.lite.Interpreter(model_path=\"yamnet_5s_mean_embedding.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# --- 3. Run inference ---\n",
    "interpreter.set_tensor(input_details[0]['index'], waveform)\n",
    "interpreter.invoke()\n",
    "\n",
    "# --- 4. Get embedding ---\n",
    "embedding = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Output shape:\", embedding.shape)  # should be (1, 1024)\n",
    "print(\"First 5 elements:\", embedding[0, :5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
